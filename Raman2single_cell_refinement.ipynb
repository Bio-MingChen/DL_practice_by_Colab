{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCaWrRVldE37Tpv3d3XkgH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bio-MingChen/DL_practice_by_Colab/blob/main/Raman2single_cell_refinement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Call clear_output() to clear the output of the current cell\n",
        "! pip install scanpy scikit-learn\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "opNKpA5e-MnT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9O2wECU-L4e",
        "outputId": "703716b4-a8bd-41da-ce6b-efd3961c1b98"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JaicFCx-20_x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.extend([\".\", \"..\"])\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils as U\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "\n",
        "# -------------------------\n",
        "# helpers\n",
        "# -------------------------\n",
        "def turn_on_model(model: nn.Module):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def turn_off_model(model: nn.Module):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Encoder / Decoder\n",
        "# -------------------------\n",
        "class StandardEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    与原文相同的多层全连接 + BN + ReLU 堆叠，最后输出 mean / logvar。\n",
        "    hidden_dim 默认 512，可改。\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):\n",
        "        super().__init__()\n",
        "        self.part1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # extra block (保持与原文一致)\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.to_mean   = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.to_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.part1(x)\n",
        "        return self.to_mean(h), self.to_logvar(h)\n",
        "\n",
        "\n",
        "class StandardDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    与原文一致，区别仅在于用显式数字替代 1<<k。\n",
        "    no_final_relu=True 时不做末层 ReLU（常用于重构原值回归）。\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512, no_final_relu: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = [\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # extra block\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "        ]\n",
        "        if not no_final_relu:\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    判别器与原文相同：可选 spectral norm，输出 end_dim=2 个 logits（源/目标 one-hot）。\n",
        "    层宽 64 -> 32 -> 32 -> end_dim，均显式数字，不再用位移写法。\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim: int, spectral: bool = True, end_dim: int = 2):\n",
        "        super().__init__()\n",
        "        def linear(n_in, n_out):\n",
        "            layer = nn.Linear(n_in, n_out)\n",
        "            return U.spectral_norm(layer) if spectral else layer\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            linear(latent_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            linear(64, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            linear(32, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            linear(32, end_dim),   # logits（不用 Sigmoid/Softmax，后面配 BCEWithLogits）\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# VAE wrapper\n",
        "# -------------------------\n",
        "class VAE(nn.Module):\n",
        "    \"\"\"\n",
        "    与原逻辑保持一致：\n",
        "      - is_vae=True 时用 reparam；False 则直接用 mean。\n",
        "      - latent_norm: 'batch'（默认）或 'none'；原代码等价于一直做 BatchNorm。\n",
        "      - forward: 先 reparam 再（可选）做 latent 归一化（去掉了原代码中永不触发的 if 0 分支）。\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder: nn.Module,\n",
        "        decoder: nn.Module,\n",
        "        is_vae: bool = True,\n",
        "        latent_norm: str = 'batch',   # 'batch' or 'none'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.is_vae = is_vae\n",
        "\n",
        "        if latent_norm == 'batch':\n",
        "            self.latent_normalizer = nn.BatchNorm1d(self.encoder.latent_dim)\n",
        "            self.use_latent_norm = True\n",
        "        elif latent_norm == 'none':\n",
        "            self.latent_normalizer = None\n",
        "            self.use_latent_norm = False\n",
        "        else:\n",
        "            raise ValueError(\"latent_norm must be 'batch' or 'none'\")\n",
        "\n",
        "    @staticmethod\n",
        "    def reparam_trick(mean, logvar, is_vae: bool):\n",
        "        if not is_vae:\n",
        "            return mean\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mean + eps * std\n",
        "\n",
        "    def get_latent(self, x, latent_noise_std: float = 0.0):\n",
        "        mean, logvar = self.encoder(x)\n",
        "        z = self.reparam_trick(mean, logvar, self.is_vae)\n",
        "        if self.use_latent_norm:\n",
        "            z = self.latent_normalizer(z)\n",
        "        if latent_noise_std and latent_noise_std > 0:\n",
        "            z = z + latent_noise_std * torch.randn_like(z)\n",
        "        return z\n",
        "\n",
        "    def forward(self, x, latent_noise_std: float = 0.0):\n",
        "        mean, logvar = self.encoder(x)\n",
        "        z = self.reparam_trick(mean, logvar, self.is_vae)\n",
        "        if self.use_latent_norm:\n",
        "            z = self.latent_normalizer(z)\n",
        "        if latent_noise_std and latent_noise_std > 0:\n",
        "            z = z + latent_noise_std * torch.randn_like(z)\n",
        "        recon_x = self.decoder(z)\n",
        "        return recon_x, mean, logvar, z\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Losses（保持原接口/语义）\n",
        "# -------------------------\n",
        "def old_mse_loss(x, recon_x, weights=None):\n",
        "    \"\"\"\n",
        "    原始实现是 MSE；去掉了会把 loss 放大的常数乘子。\n",
        "    \"\"\"\n",
        "    if weights is None:\n",
        "        return F.mse_loss(recon_x, x)\n",
        "    else:\n",
        "        # 与 weighted_mse 保持一致\n",
        "        return weighted_mse(recon_x, x, weights=weights)\n",
        "\n",
        "def weighted_mse(a, b, weights=None):\n",
        "    if weights is None:\n",
        "        return F.mse_loss(a, b)\n",
        "    return torch.sum(((a - b) ** 2) * weights)\n",
        "\n",
        "def old_vae_loss(x, recon_x, mean, logvar, weights=None, this_lambda=0.0):\n",
        "    \"\"\"\n",
        "    VAE 重构 + KL；当 this_lambda=0 时退化为纯 MSE（原文常用）。\n",
        "    \"\"\"\n",
        "    if weights is None:\n",
        "        bce = F.mse_loss(recon_x, x)\n",
        "    else:\n",
        "        bce = weighted_mse(recon_x, x, weights=weights)\n",
        "    kl_div = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
        "    return bce + this_lambda * kl_div\n",
        "\n",
        "def discrim_loss(pred_logits, true_onehot):\n",
        "    \"\"\"\n",
        "    与原代码一致：使用二分类的 two-logits + one-hot 目标。\n",
        "    pred_logits: (B, 2)\n",
        "    true_onehot: (B, 2)  例如 [1,0] or [0,1]\n",
        "    \"\"\"\n",
        "    return F.binary_cross_entropy_with_logits(pred_logits, true_onehot)\n",
        "\n",
        "def adv_vae_loss(\n",
        "    x, recon_x,\n",
        "    mean, logvar, discrim_preds,\n",
        "    alpha: float, beta: float, device=None, weights=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    组合损失：alpha * VAE重构(+KL) + beta * 判别器 BCE。\n",
        "    判别器监督标签默认使用 [1,0]（source）作为“真”标签。\n",
        "    \"\"\"\n",
        "    vae_part = old_vae_loss(x, recon_x, mean, logvar, weights=weights)\n",
        "\n",
        "    # 构造 one-hot 标签（与原文一致）\n",
        "    source_label = torch.tensor([1.0, 0.0], device=device)\n",
        "    discrim_labels = source_label.expand(x.shape[0], 2)\n",
        "\n",
        "    disc_part = F.binary_cross_entropy_with_logits(discrim_preds, discrim_labels)\n",
        "    total = alpha * vae_part + beta * disc_part\n",
        "    return total, vae_part, disc_part\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Preprocess → Balanced split → DataLoaders\n",
        "# for RNA / ATAC(gene activity) / Raman\n",
        "# ================================\n",
        "import numpy as np\n",
        "import scanpy as sc\n",
        "import torch\n",
        "import scipy.sparse as sp\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# --------- 配置 ---------\n",
        "BATCH_SIZE = 64\n",
        "TEST_FRACTION = 0.20\n",
        "RANDOM_SEED = 0\n",
        "\n",
        "# RNA\n",
        "RNA_USE_X_AS_IS = True     # True: 使用 adata_n.X（你说已是 scaled）; False: 用 layers['data'] 或 ['counts'] 再做 log1p\n",
        "RNA_USE_HVG = True         # 若 adata_n.var['highly_variable'] 存在，优先只取 HVG\n",
        "RNA_N_TOP_HVG = 2000       # 若需要重新挑HVV时可用\n",
        "RNA_ZSCORE = False         # 通常不需要（你已 scaled）\n",
        "\n",
        "# ATAC（gene activity）\n",
        "ATAC_USE_X_AS_IS = True    # True: 用 adata_a.X（已 normalized 的基因活性）\n",
        "ATAC_ZSCORE = False        # 可按基因再做 z-score（通常不需要，怕和上游norm重复）\n",
        "\n",
        "# Raman\n",
        "RAMAN_ZSCORE = True        # 对每个波数做 z-score（常见，对模型更友好）\n",
        "\n",
        "# 跨模态（RNA vs ATAC）\n",
        "USE_GENE_INTERSECTION = False  # 若要在RNA/ATAC上做对齐/共享空间，建议 True\n",
        "# -----------------------\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device =', device)\n",
        "\n",
        "# def to_dense_if_sparse(X):\n",
        "#     if hasattr(X, \"A\"):  # scipy.sparse matrix\n",
        "#         return X.A.astype(np.float32)\n",
        "#     return np.asarray(X, dtype=np.float32)\n",
        "\n",
        "def to_dense_if_sparse(X):\n",
        "    \"\"\"稳健地把 AnnData.X 或 layer 转成 float32 的 dense ndarray。\"\"\"\n",
        "    if sp.issparse(X):                 # 关键：用 issparse 判断\n",
        "        return X.toarray().astype(np.float32)\n",
        "    # 某些环境里会是 np.matrix，需要转成 ndarray\n",
        "    if isinstance(X, np.matrix):\n",
        "        return np.asarray(X, dtype=np.float32)\n",
        "    return np.asarray(X, dtype=np.float32)\n",
        "\n",
        "def zscore_features(X):\n",
        "    mu = X.mean(axis=0, keepdims=True)\n",
        "    sd = X.std(axis=0, keepdims=True) + 1e-6\n",
        "    return (X - mu) / sd\n",
        "\n",
        "def encode_labels(celltypes):\n",
        "    classes = np.unique(celltypes)\n",
        "    label2id = {c:i for i,c in enumerate(classes)}\n",
        "    id2label = {i:c for c,i in label2id.items()}\n",
        "    y = np.array([label2id[c] for c in celltypes], dtype=np.int64)\n",
        "    return y, label2id, id2label\n",
        "\n",
        "def balanced_stratified_indices(labels, test_fraction=0.2, seed=0):\n",
        "    rng = check_random_state(seed)\n",
        "    labels = np.asarray(labels)\n",
        "    classes, counts = np.unique(labels, return_counts=True)\n",
        "    n_per_class = counts.min()\n",
        "    train_idx, test_idx = [], []\n",
        "    for c in classes:\n",
        "        idx_c = np.where(labels == c)[0]\n",
        "        idx_c = rng.permutation(idx_c)[:n_per_class]\n",
        "        n_test = int(round(test_fraction * n_per_class))\n",
        "        test_idx.extend(idx_c[:n_test])\n",
        "        train_idx.extend(idx_c[n_test:])\n",
        "    return np.array(train_idx), np.array(test_idx)\n",
        "\n",
        "def make_loaders(X, y, batch_size=64, shuffle_train=True):\n",
        "    Xt = torch.from_numpy(X).float()\n",
        "    yt = torch.from_numpy(y).long()\n",
        "    ds = TensorDataset(Xt, yt)\n",
        "    train = shuffle_train\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=train), ds\n",
        "\n",
        "# ---------- 读你的 AnnData ----------\n",
        "adata_a = sc.read_h5ad('/content/drive/MyDrive/Colab Notebooks/data/Raman_single_cell/adata_a.h5ad') # gene activity\n",
        "adata_n = sc.read_h5ad('/content/drive/MyDrive/Colab Notebooks/data/Raman_single_cell/adata_n.h5ad') # RNA\n",
        "adata_r = sc.read_h5ad('/content/drive/MyDrive/Colab Notebooks/data/Raman_single_cell/adata_r.h5ad') # Raman\n",
        "\n",
        "assert 'cell_type' in adata_a.obs.columns\n",
        "assert 'cell_type' in adata_n.obs.columns\n",
        "assert 'cell_type' in adata_r.obs.columns\n",
        "\n",
        "# =========================\n",
        "# 1) RNA\n",
        "# =========================\n",
        "if RNA_USE_X_AS_IS:\n",
        "    Xn = to_dense_if_sparse(adata_n.X)\n",
        "else:\n",
        "    # 如需改用 layers['data'] 或 counts：示例\n",
        "    if 'data' in adata_n.layers:\n",
        "        Xn = to_dense_if_sparse(adata_n.layers['data'])\n",
        "    elif 'counts' in adata_n.layers:\n",
        "        Xn = to_dense_if_sparse(adata_n.layers['counts'])\n",
        "        Xn = np.log1p(Xn)\n",
        "    else:\n",
        "        Xn = to_dense_if_sparse(adata_n.X)\n",
        "\n",
        "gene_names_rna = np.array(adata_n.var_names)\n",
        "# 只取 HVG（若提供）\n",
        "if RNA_USE_HVG and ('highly_variable' in adata_n.var.columns):\n",
        "    hvg_mask = adata_n.var['highly_variable'].values\n",
        "    Xn = Xn[:, hvg_mask]\n",
        "    gene_names_rna = gene_names_rna[hvg_mask]\n",
        "\n",
        "if RNA_ZSCORE:\n",
        "    Xn = zscore_features(Xn)\n",
        "\n",
        "yn, n_label2id, n_id2label = encode_labels(adata_n.obs['cell_type'].values)\n",
        "\n",
        "# =========================\n",
        "# 2) ATAC (gene activity)\n",
        "# =========================\n",
        "if ATAC_USE_X_AS_IS:\n",
        "    Xa = to_dense_if_sparse(adata_a.X)\n",
        "else:\n",
        "    Xa = to_dense_if_sparse(adata_a.X)  # 这里留钩子：如需换层/再log，可改这里\n",
        "\n",
        "gene_names_atac = np.array(adata_a.var_names)\n",
        "\n",
        "if ATAC_ZSCORE:\n",
        "    Xa = zscore_features(Xa)\n",
        "\n",
        "ya, a_label2id, a_id2label = encode_labels(adata_a.obs['cell_type'].values)\n",
        "\n",
        "# =========================\n",
        "# 3) 跨模态基因集合对齐（可选）\n",
        "# =========================\n",
        "if USE_GENE_INTERSECTION:\n",
        "    common_genes, rn_idx, aa_idx = np.intersect1d(gene_names_rna, gene_names_atac, return_indices=True)\n",
        "    if common_genes.size == 0:\n",
        "        raise ValueError(\"RNA 与 ATAC 的基因集合没有交集，请检查 var_names。\")\n",
        "    Xn = Xn[:, rn_idx]\n",
        "    Xa = Xa[:, aa_idx]\n",
        "    gene_names_rna = common_genes\n",
        "    gene_names_atac = common_genes\n",
        "    print(f\"[Gene Intersection] common genes: {len(common_genes)}\")\n",
        "else:\n",
        "    print(\"[Gene Intersection] disabled: RNA/ATAC 特征不强制一致\")\n",
        "\n",
        "# =========================\n",
        "# 4) Raman\n",
        "# =========================\n",
        "Xr = to_dense_if_sparse(adata_r.X)  # normalized spectra\n",
        "if RAMAN_ZSCORE:\n",
        "    Xr = zscore_features(Xr)\n",
        "\n",
        "yr, r_label2id, r_id2label = encode_labels(adata_r.obs['cell_type'].values)\n",
        "\n",
        "# =========================\n",
        "# 5) 分层均衡 + 划分 + DataLoaders\n",
        "# =========================\n",
        "# RNA\n",
        "n_train_idx, n_test_idx = balanced_stratified_indices(yn, TEST_FRACTION, RANDOM_SEED)\n",
        "Xn_train, Xn_test = Xn[n_train_idx], Xn[n_test_idx]\n",
        "yn_train, yn_test = yn[n_train_idx], yn[n_test_idx]\n",
        "rna_train_loader, rna_train_ds = make_loaders(Xn_train, yn_train, BATCH_SIZE, shuffle_train=True)\n",
        "rna_test_loader,  rna_test_ds  = make_loaders(Xn_test,  yn_test,  BATCH_SIZE, shuffle_train=False)\n",
        "print(f\"[RNA] X:{Xn.shape}  train:{len(rna_train_ds)}  test:{len(rna_test_ds)}  classes:{len(n_label2id)}\")\n",
        "\n",
        "# ATAC\n",
        "a_train_idx, a_test_idx = balanced_stratified_indices(ya, TEST_FRACTION, RANDOM_SEED)\n",
        "Xa_train, Xa_test = Xa[a_train_idx], Xa[a_test_idx]\n",
        "ya_train, ya_test = ya[a_train_idx], ya[a_test_idx]\n",
        "atac_train_loader, atac_train_ds = make_loaders(Xa_train, ya_train, BATCH_SIZE, shuffle_train=True)\n",
        "atac_test_loader,  atac_test_ds  = make_loaders(Xa_test,  ya_test,  BATCH_SIZE, shuffle_train=False)\n",
        "print(f\"[ATAC(gene activity)] X:{Xa.shape}  train:{len(atac_train_ds)}  test:{len(atac_test_ds)}  classes:{len(a_label2id)}\")\n",
        "\n",
        "# Raman\n",
        "r_train_idx, r_test_idx = balanced_stratified_indices(yr, TEST_FRACTION, RANDOM_SEED)\n",
        "Xr_train, Xr_test = Xr[r_train_idx], Xr[r_test_idx]\n",
        "yr_train, yr_test = yr[r_train_idx], yr[r_test_idx]\n",
        "raman_train_loader, raman_train_ds = make_loaders(Xr_train, yr_train, BATCH_SIZE, shuffle_train=True)\n",
        "raman_test_loader,  raman_test_ds  = make_loaders(Xr_test,  yr_test,  BATCH_SIZE, shuffle_train=False)\n",
        "print(f\"[RAMAN] X:{Xr.shape}  train:{len(raman_train_ds)}  test:{len(raman_test_ds)}  classes:{len(r_label2id)}\")\n",
        "\n",
        "# =========================\n",
        "# 6) 打包输出（方便后续使用）\n",
        "# =========================\n",
        "preprocessed = {\n",
        "    \"rna\": {\n",
        "        \"train_loader\": rna_train_loader,\n",
        "        \"test_loader\":  rna_test_loader,\n",
        "        \"y_map\": {\"label2id\": n_label2id, \"id2label\": n_id2label},\n",
        "        \"feature_names\": gene_names_rna,\n",
        "        \"shape\": Xn.shape,\n",
        "    },\n",
        "    \"atac\": {\n",
        "        \"train_loader\": atac_train_loader,\n",
        "        \"test_loader\":  atac_test_loader,\n",
        "        \"y_map\": {\"label2id\": a_label2id, \"id2label\": a_id2label},\n",
        "        \"feature_names\": gene_names_atac,\n",
        "        \"shape\": Xa.shape,\n",
        "    },\n",
        "    \"raman\": {\n",
        "        \"train_loader\": raman_train_loader,\n",
        "        \"test_loader\":  raman_test_loader,\n",
        "        \"y_map\": {\"label2id\": r_label2id, \"id2label\": r_id2label},\n",
        "        \"feature_names\": np.array(adata_r.var_names, dtype=str),\n",
        "        \"shape\": Xr.shape,\n",
        "    }\n",
        "}\n",
        "\n",
        "print(preprocessed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDkh2tPS5uMS",
        "outputId": "2c06875c-7c96-4959-a45d-511650cdb774"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device = cpu\n",
            "[Gene Intersection] disabled: RNA/ATAC 特征不强制一致\n",
            "[RNA] X:(938, 2000)  train:492  test:124  classes:4\n",
            "[ATAC(gene activity)] X:(10738, 19039)  train:2660  test:664  classes:4\n",
            "[RAMAN] X:(1415, 432)  train:1040  test:260  classes:4\n",
            "{'rna': {'train_loader': <torch.utils.data.dataloader.DataLoader object at 0x794c7c079d10>, 'test_loader': <torch.utils.data.dataloader.DataLoader object at 0x794cb39c1490>, 'y_map': {'label2id': {'HSC': 0, 'Naive B': 1, 'Pre B': 2, 'Pro B': 3}, 'id2label': {0: 'HSC', 1: 'Naive B', 2: 'Pre B', 3: 'Pro B'}}, 'feature_names': array(['FAM132A', 'ACAP3', 'DVL1', ..., 'C21orf58', 'DIP2A', 'AC145212.2'],\n",
            "      dtype=object), 'shape': (938, 2000)}, 'atac': {'train_loader': <torch.utils.data.dataloader.DataLoader object at 0x794c7c07ce50>, 'test_loader': <torch.utils.data.dataloader.DataLoader object at 0x794c7c072890>, 'y_map': {'label2id': {'HSC': 0, 'Naive B': 1, 'Pre B': 2, 'Pro B': 3}, 'id2label': {0: 'HSC', 1: 'Naive B', 2: 'Pre B', 3: 'Pro B'}}, 'feature_names': array(['OR4F5', 'OR4F29', 'OR4F16', ..., 'DAZ4', 'BPY2C', 'CDY1'],\n",
            "      dtype=object), 'shape': (10738, 19039)}, 'raman': {'train_loader': <torch.utils.data.dataloader.DataLoader object at 0x794c7c070850>, 'test_loader': <torch.utils.data.dataloader.DataLoader object at 0x794c7c070390>, 'y_map': {'label2id': {'HSC': 0, 'Naive B': 1, 'Pre B': 2, 'Pro B': 3}, 'id2label': {0: 'HSC', 1: 'Naive B', 2: 'Pre B', 3: 'Pro B'}}, 'feature_names': array(['602.022', '605.013', '608.003', '610.992', '613.98', '616.965',\n",
            "       '619.951', '622.936', '625.92', '628.902', '631.884', '634.865',\n",
            "       '637.843', '640.822', '643.799', '646.776', '649.75', '652.725',\n",
            "       '655.698', '658.671', '661.641', '664.611', '667.581', '670.549',\n",
            "       '673.515', '676.483', '679.447', '682.411', '685.374', '688.335',\n",
            "       '691.296', '694.256', '697.214', '700.172', '703.129', '706.085',\n",
            "       '709.04', '711.994', '714.946', '717.896', '720.847', '723.797',\n",
            "       '726.747', '729.693', '732.64', '735.586', '738.529', '741.474',\n",
            "       '744.415', '747.357', '750.297', '753.235', '756.175', '759.111',\n",
            "       '762.047', '764.983', '767.917', '770.85', '773.782', '776.712',\n",
            "       '779.644', '782.573', '785.501', '788.428', '791.354', '794.279',\n",
            "       '797.203', '800.125', '803.047', '805.968', '808.889', '811.808',\n",
            "       '814.725', '817.642', '820.559', '823.472', '826.387', '829.299',\n",
            "       '832.211', '835.121', '838.032', '840.94', '843.847', '846.755',\n",
            "       '849.661', '852.565', '855.47', '858.372', '861.274', '864.174',\n",
            "       '867.074', '869.972', '872.87', '875.767', '878.662', '881.557',\n",
            "       '884.451', '887.344', '890.234', '893.125', '896.016', '898.903',\n",
            "       '901.791', '904.679', '907.563', '910.449', '913.331', '916.215',\n",
            "       '919.096', '921.977', '924.856', '927.734', '930.613', '933.489',\n",
            "       '936.365', '939.239', '942.112', '944.985', '947.856', '950.727',\n",
            "       '953.596', '956.465', '959.333', '962.2', '965.066', '967.931',\n",
            "       '970.793', '973.656', '976.519', '979.378', '982.239', '985.096',\n",
            "       '987.955', '990.81', '993.667', '996.521', '999.376', '1002.23',\n",
            "       '1005.08', '1007.93', '1010.78', '1013.63', '1016.48', '1019.32',\n",
            "       '1022.17', '1025.01', '1027.86', '1030.7', '1033.54', '1036.38',\n",
            "       '1039.22', '1042.06', '1044.9', '1047.74', '1050.57', '1053.41',\n",
            "       '1056.24', '1059.08', '1061.91', '1064.74', '1067.57', '1070.4',\n",
            "       '1073.23', '1076.05', '1078.88', '1081.7', '1084.53', '1087.35',\n",
            "       '1090.17', '1092.99', '1095.81', '1098.63', '1101.45', '1104.27',\n",
            "       '1107.08', '1109.9', '1112.71', '1115.53', '1118.34', '1121.15',\n",
            "       '1123.96', '1126.77', '1129.58', '1132.39', '1135.19', '1138.0',\n",
            "       '1140.8', '1143.61', '1146.41', '1149.21', '1152.01', '1154.81',\n",
            "       '1157.61', '1160.41', '1163.2', '1166.0', '1168.8', '1171.59',\n",
            "       '1174.38', '1177.17', '1179.96', '1182.76', '1185.54', '1188.33',\n",
            "       '1191.12', '1193.91', '1196.69', '1199.47', '1202.26', '1205.04',\n",
            "       '1207.82', '1210.6', '1213.38', '1216.16', '1218.94', '1221.71',\n",
            "       '1224.49', '1227.26', '1230.04', '1232.81', '1235.58', '1238.35',\n",
            "       '1241.12', '1243.89', '1246.66', '1249.43', '1252.19', '1254.95',\n",
            "       '1257.72', '1260.48', '1263.24', '1266.01', '1268.77', '1271.52',\n",
            "       '1274.28', '1277.04', '1279.8', '1282.55', '1285.31', '1288.06',\n",
            "       '1290.81', '1293.56', '1296.31', '1299.06', '1301.81', '1304.56',\n",
            "       '1307.31', '1310.05', '1312.8', '1315.54', '1318.28', '1321.03',\n",
            "       '1323.77', '1326.51', '1329.25', '1331.99', '1334.72', '1337.46',\n",
            "       '1340.19', '1342.93', '1345.66', '1348.39', '1351.13', '1353.85',\n",
            "       '1356.59', '1359.31', '1362.04', '1364.77', '1367.49', '1370.22',\n",
            "       '1372.94', '1375.67', '1378.39', '1381.11', '1383.83', '1386.55',\n",
            "       '1389.27', '1391.98', '1394.7', '1397.42', '1400.13', '1402.84',\n",
            "       '1405.56', '1408.27', '1410.98', '1413.69', '1416.4', '1419.11',\n",
            "       '1421.81', '1424.52', '1427.22', '1429.93', '1432.63', '1435.33',\n",
            "       '1438.04', '1440.74', '1443.44', '1446.13', '1448.83', '1451.53',\n",
            "       '1454.22', '1456.92', '1459.61', '1462.3', '1465.0', '1467.69',\n",
            "       '1470.38', '1473.07', '1475.76', '1478.44', '1481.13', '1483.81',\n",
            "       '1486.5', '1489.18', '1491.87', '1494.55', '1497.23', '1499.91',\n",
            "       '1502.59', '1505.26', '1507.94', '1510.62', '1513.29', '1515.97',\n",
            "       '1518.64', '1521.31', '1523.98', '1526.65', '1529.32', '1531.99',\n",
            "       '1534.66', '1537.33', '1539.99', '1542.66', '1545.32', '1547.99',\n",
            "       '1550.65', '1553.31', '1555.97', '1558.63', '1561.29', '1563.95',\n",
            "       '1566.6', '1569.26', '1571.91', '1574.57', '1577.22', '1579.87',\n",
            "       '1582.53', '1585.18', '1587.82', '1590.47', '1593.12', '1595.77',\n",
            "       '1598.41', '1601.06', '1603.7', '1606.34', '1608.98', '1611.63',\n",
            "       '1614.27', '1616.91', '1619.54', '1622.18', '1624.82', '1627.45',\n",
            "       '1630.09', '1632.72', '1635.36', '1637.99', '1640.62', '1643.25',\n",
            "       '1645.88', '1648.51', '1651.13', '1653.76', '1656.38', '1659.01',\n",
            "       '1661.63', '1664.26', '1666.88', '1669.5', '1672.12', '1674.74',\n",
            "       '1677.36', '1679.97', '1682.59', '1685.21', '1687.82', '1690.43',\n",
            "       '1693.05', '1695.66', '1698.27', '1700.88', '1703.49', '1706.1',\n",
            "       '1708.71', '1711.31', '1713.92', '1716.52', '1719.13', '1721.73',\n",
            "       '1724.33', '1726.93', '1729.53', '1732.13', '1734.73', '1737.33',\n",
            "       '1739.92', '1742.52', '1745.11', '1747.71', '1750.3', '1752.89',\n",
            "       '1755.49', '1758.08', '1760.66', '1763.25', '1765.84', '1768.43',\n",
            "       '1771.01', '1773.6', '1776.18', '1778.76', '1781.35', '1783.93',\n",
            "       '1786.51', '1789.09', '1791.67', '1794.25', '1796.82', '1799.4'],\n",
            "      dtype='<U7'), 'shape': (1415, 432)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Train ref_vae for RNA & ATAC (gene activity)\n",
        "# =========================\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# ---- 引入你之前整理好的模型/损失（保持与文献一致的结构） ----\n",
        "# 请确保以下类/函数已在环境中（来自我们上一条“基础模型代码”）:\n",
        "# StandardEncoder, StandardDecoder, VAE, old_vae_loss\n",
        "# 如果不在，请把那段模型代码粘过来。\n",
        "\n",
        "# -----------------------\n",
        "# 可调超参\n",
        "# -----------------------\n",
        "LATENT_DIM  = 128\n",
        "HIDDEN_DIM  = 2048\n",
        "EPOCHS      = 30\n",
        "BATCH_SIZE  = 128\n",
        "LR          = 1e-4          # 文献里用过 1e-5/1e-4，先用 1e-4 更快收敛\n",
        "KL_WEIGHT   = 1e-4          # KL 的权重（0 时退化为纯 AE）\n",
        "NUM_WORKERS = 0\n",
        "PIN_MEMORY  = True\n",
        "\n",
        "USE_RNA_HVG = True          # 若 adata_n.var['highly_variable'] 存在，优先只取 HVG\n",
        "RANDOM_SEED = 0\n",
        "TEST_FRACTION = 0.2         # 8:2 划分\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device =', device)\n",
        "\n",
        "# -----------------------\n",
        "# 小工具\n",
        "# -----------------------\n",
        "def to_dense_if_sparse(X):\n",
        "    if sp.issparse(X):\n",
        "        return X.toarray().astype(np.float32)\n",
        "    # 兼容 np.matrix\n",
        "    return np.asarray(X, dtype=np.float32)\n",
        "\n",
        "def balanced_stratified_indices(labels, test_fraction=0.2, seed=0):\n",
        "    rng = check_random_state(seed)\n",
        "    labels = np.asarray(labels)\n",
        "    classes, counts = np.unique(labels, return_counts=True)\n",
        "    n_per = counts.min()\n",
        "    tr, te = [], []\n",
        "    for c in classes:\n",
        "        idx = np.where(labels == c)[0]\n",
        "        idx = rng.permutation(idx)[:n_per]\n",
        "        n_test = int(round(test_fraction * n_per))\n",
        "        te.extend(idx[:n_test]); tr.extend(idx[n_test:])\n",
        "    return np.array(tr), np.array(te)\n",
        "\n",
        "def make_loader_X(X, batch_size=128, shuffle=True):\n",
        "    Xt = torch.from_numpy(X).float()\n",
        "    ds = TensorDataset(Xt)  # 只有 X，没有 y\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=shuffle,\n",
        "                        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    return loader, ds\n",
        "\n",
        "def build_ref_vae(input_dim, latent_dim=128, hidden_dim=2048, kl_weight=1e-4, lr=1e-4):\n",
        "    vae = VAE(\n",
        "        encoder=StandardEncoder(input_dim, latent_dim, hidden_dim=hidden_dim),\n",
        "        decoder=StandardDecoder(input_dim, latent_dim, hidden_dim=hidden_dim, no_final_relu=True),\n",
        "        is_vae=True,                 # 开启 VAE（reparam + KL）\n",
        "        latent_norm='batch',         # 与原始代码一致：latent 做 BN（可改 'none'）\n",
        "    ).to(device)\n",
        "    opt = torch.optim.Adam(vae.parameters(), lr=lr)\n",
        "    return vae, opt\n",
        "\n",
        "def train_ref_vae(name, X_train, X_test, epochs=30, kl_weight=1e-4, lr=1e-4):\n",
        "    in_dim = X_train.shape[1]\n",
        "    vae, opt = build_ref_vae(in_dim, LATENT_DIM, HIDDEN_DIM, kl_weight, lr)\n",
        "\n",
        "    train_loader, _ = make_loader_X(X_train, BATCH_SIZE, shuffle=True)\n",
        "    test_loader,  _ = make_loader_X(X_test,  BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print(f\"\\n=== Train {name}_ref_vae ===\")\n",
        "    for ep in range(1, epochs+1):\n",
        "        vae.train(); loss_sum = 0.0; n = 0\n",
        "        for (xb,) in train_loader:\n",
        "            xb = xb.to(device)\n",
        "            opt.zero_grad()\n",
        "            recon, mean, logvar, _ = vae(xb)\n",
        "            loss = old_vae_loss(xb, recon, mean, logvar, weights=None, this_lambda=kl_weight)\n",
        "            loss.backward(); opt.step()\n",
        "            loss_sum += loss.item(); n += xb.size(0)\n",
        "        tr_loss = loss_sum / n\n",
        "\n",
        "        # 简单的 test 重构评估（不回传）\n",
        "        vae.eval(); te_sum = 0.0; m = 0\n",
        "        with torch.no_grad():\n",
        "            for (xb,) in test_loader:\n",
        "                xb = xb.to(device)\n",
        "                recon, mean, logvar, _ = vae(xb)\n",
        "                loss = old_vae_loss(xb, recon, mean, logvar, weights=None, this_lambda=kl_weight)\n",
        "                te_sum += loss.item(); m += xb.size(0)\n",
        "        te_loss = te_sum / m\n",
        "        print(f\"[{name} ep {ep:02d}] train_loss={tr_loss:.6f}  test_loss={te_loss:.6f}\")\n",
        "\n",
        "    return vae\n",
        "\n",
        "# -----------------------\n",
        "# 1) RNA → rna_ref_vae\n",
        "# -----------------------\n",
        "# X：优先使用 adata_n.X（你说是 scaled data）\n",
        "Xn = to_dense_if_sparse(adata_n.X)\n",
        "if USE_RNA_HVG and ('highly_variable' in adata_n.var.columns):\n",
        "    hvg = adata_n.var['highly_variable'].values\n",
        "    if hvg.sum() > 0:\n",
        "        Xn = Xn[:, hvg]\n",
        "        print(f\"[RNA] use HVG: {hvg.sum()} genes\")\n",
        "\n",
        "# 分层均衡划分（只用 label 来划分，不参与训练）\n",
        "yn = np.array(adata_n.obs['cell_type'])\n",
        "# 映射到 int 以便分层\n",
        "classes = {c:i for i,c in enumerate(np.unique(yn))}\n",
        "yn_id = np.array([classes[c] for c in yn], dtype=np.int64)\n",
        "n_tr_idx, n_te_idx = balanced_stratified_indices(yn_id, TEST_FRACTION, RANDOM_SEED)\n",
        "Xn_train, Xn_test = Xn[n_tr_idx], Xn[n_te_idx]\n",
        "\n",
        "rna_ref_vae = train_ref_vae(\"RNA\", Xn_train, Xn_test,\n",
        "                            epochs=EPOCHS, kl_weight=KL_WEIGHT, lr=LR)\n",
        "\n",
        "# 可选：保存\n",
        "torch.save(rna_ref_vae.state_dict(), \"rna_ref_vae.pt\")\n",
        "print(\"Saved rna_ref_vae.pt\")\n",
        "\n",
        "# -----------------------\n",
        "# 2) ATAC(gene activity) → atac_ref_vae\n",
        "# -----------------------\n",
        "Xa = to_dense_if_sparse(adata_a.X)  # gene activity，特征是基因\n",
        "ya = np.array(adata_a.obs['cell_type'])\n",
        "classes_a = {c:i for i,c in enumerate(np.unique(ya))}\n",
        "ya_id = np.array([classes_a[c] for c in ya], dtype=np.int64)\n",
        "a_tr_idx, a_te_idx = balanced_stratified_indices(ya_id, TEST_FRACTION, RANDOM_SEED)\n",
        "Xa_train, Xa_test = Xa[a_tr_idx], Xa[a_te_idx]\n",
        "\n",
        "atac_ref_vae = train_ref_vae(\"ATAC\", Xa_train, Xa_test,\n",
        "                             epochs=EPOCHS, kl_weight=KL_WEIGHT, lr=LR)\n",
        "\n",
        "torch.save(atac_ref_vae.state_dict(), \"atac_ref_vae.pt\")\n",
        "print(\"Saved atac_ref_vae.pt\")\n",
        "\n",
        "# -----------------------\n",
        "# 用法示例：取 latent（与文献接口一致）\n",
        "# -----------------------\n",
        "rna_ref_vae.eval(); atac_ref_vae.eval()\n",
        "with torch.no_grad():\n",
        "    # RNA latent\n",
        "    rna_latent = rna_ref_vae.get_latent(torch.from_numpy(Xn_test).to(device)).cpu().numpy()\n",
        "    # ATAC latent\n",
        "    atac_latent = atac_ref_vae.get_latent(torch.from_numpy(Xa_test).to(device)).cpu().numpy()\n",
        "print(\"Latent shapes -> RNA:\", rna_latent.shape, \" ATAC:\", atac_latent.shape)\n"
      ],
      "metadata": {
        "id": "SRLYfgfsCA7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 1) 准备 RNA 的潜变量与标签（用之前的 rna_ref_vae、Xn_train/Xn_test）\n",
        "rna_ref_vae.eval()\n",
        "\n",
        "def get_latent_batches(model, X, batch=256):\n",
        "    Z = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(X), batch):\n",
        "            xb = torch.from_numpy(X[i:i+batch]).float().to(device)\n",
        "            z = model.get_latent(xb)\n",
        "            Z.append(z.cpu().numpy())\n",
        "    return np.vstack(Z)\n",
        "\n",
        "Z_train = get_latent_batches(rna_ref_vae, Xn_train)\n",
        "Z_test  = get_latent_batches(rna_ref_vae, Xn_test)\n",
        "\n",
        "# 标签（用你之前 balanced split 的 yn_train/yn_test；如果没有，就用 adata_n.obs['cell_type'] 映射）\n",
        "y_train = yn_train\n",
        "y_test  = yn_test\n",
        "n_classes = int(max(yn_train.max(), yn_test.max()) + 1)\n",
        "\n",
        "# 2) 一个小 MLP 分类器（也可以用我们之前的 Discriminator(end_dim=n_classes)）\n",
        "class LatentClassifier(nn.Module):\n",
        "    def __init__(self, in_dim, n_classes, hidden=256):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden), nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden, hidden//2), nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden//2, n_classes)   # 直接输出 logits\n",
        "        )\n",
        "    def forward(self, z): return self.net(z)\n",
        "\n",
        "clf = LatentClassifier(in_dim=Z_train.shape[1], n_classes=n_classes).to(device)\n",
        "opt = torch.optim.Adam(clf.parameters(), lr=1e-3)\n",
        "\n",
        "# 类别加权（可选）\n",
        "class_counts = np.bincount(y_train, minlength=n_classes)\n",
        "weights = (len(y_train) / (class_counts + 1e-6)).astype(np.float32)\n",
        "w_t = torch.tensor(weights, dtype=torch.float32, device=device)\n",
        "criterion = nn.CrossEntropyLoss(weight=w_t)\n",
        "\n",
        "# 3) DataLoader\n",
        "train_loader = DataLoader(TensorDataset(torch.from_numpy(Z_train).float(),\n",
        "                                        torch.from_numpy(y_train).long()),\n",
        "                          batch_size=128, shuffle=True)\n",
        "test_loader  = DataLoader(TensorDataset(torch.from_numpy(Z_test).float(),\n",
        "                                        torch.from_numpy(y_test).long()),\n",
        "                          batch_size=256, shuffle=False)\n",
        "\n",
        "# 4) 训练\n",
        "EPOCHS = 20\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    clf.train(); losses=[]\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = clf(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward(); opt.step()\n",
        "        losses.append(loss.item())\n",
        "    print(f\"[clf ep {ep:02d}] loss={np.mean(losses):.4f}\")\n",
        "\n",
        "# 5) 评估\n",
        "def eval_loader(model, loader):\n",
        "    model.eval(); all_y=[]; all_p=[]\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            pred = model(xb).argmax(dim=1).cpu().numpy()\n",
        "            all_p.append(pred); all_y.append(yb.numpy())\n",
        "    y = np.concatenate(all_y); p = np.concatenate(all_p)\n",
        "    return accuracy_score(y, p), f1_score(y, p, average='macro'), confusion_matrix(y, p)\n",
        "\n",
        "acc_tr, f1_tr, _ = eval_loader(clf, train_loader)\n",
        "acc_te, f1_te, cm = eval_loader(clf, test_loader)\n",
        "print(f\"Train acc={acc_tr:.3f}  macroF1={f1_tr:.3f}\")\n",
        "print(f\"Test  acc={acc_te:.3f}  macroF1={f1_te:.3f}\")\n",
        "print(\"Confusion matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "id": "-wP9cGR7Gge0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "# ===== 小工具：批量取 latent =====\n",
        "@torch.no_grad()\n",
        "def get_latent_batches(ref_vae, X, device=None, batch_size=256):\n",
        "    device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "    ref_vae.eval()\n",
        "    Z = []\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        xb = torch.from_numpy(X[i:i+batch_size]).float().to(device)\n",
        "        z = ref_vae.get_latent(xb)\n",
        "        Z.append(z.detach().cpu().numpy())\n",
        "    return np.vstack(Z)\n",
        "\n",
        "# ===== 默认分类头（MLP）；也可传自定义/Discriminator =====\n",
        "class LatentClassifier(nn.Module):\n",
        "    def __init__(self, in_dim, n_classes, hidden=256):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden), nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden, hidden//2), nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden//2, n_classes)  # logits\n",
        "        )\n",
        "    def forward(self, z): return self.net(z)\n",
        "\n",
        "def _build_classifier(in_dim, n_classes, hidden=256, use_discriminator=False, discriminator_class=None):\n",
        "    if use_discriminator:\n",
        "        assert discriminator_class is not None, \"use_discriminator=True 需要传 discriminator_class（如你之前的 Discriminator）\"\n",
        "        # 你的 Discriminator(latent_dim, end_dim=n_classes)\n",
        "        return discriminator_class(in_dim, end_dim=n_classes)\n",
        "    else:\n",
        "        return LatentClassifier(in_dim, n_classes, hidden=hidden)\n",
        "\n",
        "# ===== 训练 + 评估（可复用到 RNA / ATAC）=====\n",
        "def train_latent_classifier(\n",
        "    ref_vae,\n",
        "    X_train, y_train,\n",
        "    X_test,  y_test,\n",
        "    *,\n",
        "    device=None,\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    lr=1e-3,\n",
        "    hidden=256,\n",
        "    class_weighting=True,\n",
        "    use_discriminator=False,\n",
        "    discriminator_class=None,   # 例如传你之前的 Discriminator 类\n",
        "    return_latents=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    在 ref_vae 潜空间上训练细胞类型分类器。\n",
        "    - ref_vae: 已训练好的 VAE/AE，需实现 .get_latent(x)\n",
        "    - X_train/X_test: numpy array (N, F)\n",
        "    - y_train/y_test: numpy int labels，从 0..K-1\n",
        "    \"\"\"\n",
        "    device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "\n",
        "    # 1) 取 latent\n",
        "    Z_train = get_latent_batches(ref_vae, X_train, device=device, batch_size=max(256, batch_size))\n",
        "    Z_test  = get_latent_batches(ref_vae, X_test,  device=device, batch_size=max(256, batch_size))\n",
        "\n",
        "    in_dim = Z_train.shape[1]\n",
        "    n_classes = int(max(y_train.max(), y_test.max()) + 1)\n",
        "\n",
        "    # 2) 分类器\n",
        "    clf = _build_classifier(in_dim, n_classes, hidden=hidden,\n",
        "                            use_discriminator=use_discriminator,\n",
        "                            discriminator_class=discriminator_class).to(device)\n",
        "    opt = torch.optim.Adam(clf.parameters(), lr=lr)\n",
        "\n",
        "    # 类别加权（可选）\n",
        "    if class_weighting:\n",
        "        counts = np.bincount(y_train, minlength=n_classes)\n",
        "        weights = (len(y_train) / (counts + 1e-6)).astype(np.float32)\n",
        "        w_t = torch.tensor(weights, dtype=torch.float32, device=device)\n",
        "        criterion = nn.CrossEntropyLoss(weight=w_t)\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 3) DataLoader（在 latent 空间上训练，速度很快）\n",
        "    train_loader = DataLoader(\n",
        "        TensorDataset(torch.from_numpy(Z_train).float(), torch.from_numpy(y_train).long()),\n",
        "        batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        TensorDataset(torch.from_numpy(Z_test).float(), torch.from_numpy(y_test).long()),\n",
        "        batch_size=max(256, batch_size), shuffle=False\n",
        "    )\n",
        "\n",
        "    # 4) 训练\n",
        "    for ep in range(1, epochs+1):\n",
        "        clf.train(); losses=[]\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits = clf(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward(); opt.step()\n",
        "            losses.append(loss.item())\n",
        "        print(f\"[latent-clf ep {ep:02d}] loss={np.mean(losses):.4f}\")\n",
        "\n",
        "    # 5) 评估\n",
        "    @torch.no_grad()\n",
        "    def _eval(loader):\n",
        "        clf.eval(); all_y=[]; all_p=[]\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            pred = clf(xb).argmax(dim=1).cpu().numpy()\n",
        "            all_p.append(pred); all_y.append(yb.numpy())\n",
        "        y = np.concatenate(all_y); p = np.concatenate(all_p)\n",
        "        return accuracy_score(y, p), f1_score(y, p, average='macro'), confusion_matrix(y, p)\n",
        "\n",
        "    acc_tr, f1_tr, _ = _eval(train_loader)\n",
        "    acc_te, f1_te, cm = _eval(test_loader)\n",
        "    print(f\"Train acc={acc_tr:.3f}  macroF1={f1_tr:.3f}\")\n",
        "    print(f\"Test  acc={acc_te:.3f}  macroF1={f1_te:.3f}\")\n",
        "\n",
        "    out = {\n",
        "        \"classifier\": clf,\n",
        "        \"metrics\": {\n",
        "            \"train_acc\": acc_tr, \"train_macro_f1\": f1_tr,\n",
        "            \"test_acc\": acc_te,  \"test_macro_f1\": f1_te,\n",
        "            \"confusion_matrix\": cm\n",
        "        }\n",
        "    }\n",
        "    if return_latents:\n",
        "        out[\"Z_train\"] = Z_train\n",
        "        out[\"Z_test\"]  = Z_test\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "GInDGvtIGhmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 假定你已经有：rna_ref_vae, Xn_train, yn_train, Xn_test, yn_test\n",
        "res_rna = train_latent_classifier(\n",
        "    rna_ref_vae, Xn_train, yn_train, Xn_test, yn_test,\n",
        "    epochs=20, batch_size=128, lr=1e-3, hidden=256,\n",
        "    class_weighting=True, use_discriminator=False,  # 或 True + 传 discriminator_class\n",
        "    return_latents=False\n",
        ")\n",
        "rna_clf = res_rna[\"classifier\"]\n"
      ],
      "metadata": {
        "id": "5VT0MU8WHaOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 假定你已经有：atac_ref_vae, Xa_train, ya_train, Xa_test, ya_test\n",
        "res_atac = train_latent_classifier(\n",
        "    atac_ref_vae, Xa_train, ya_train, Xa_test, ya_test,\n",
        "    epochs=20, batch_size=128, lr=1e-3, hidden=256,\n",
        "    class_weighting=True, use_discriminator=False\n",
        ")\n",
        "atac_clf = res_atac[\"classifier\"]\n"
      ],
      "metadata": {
        "id": "KyKYshFnHbx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from itertools import cycle\n",
        "\n",
        "# 依赖：你前面已经定义过的模块/函数：\n",
        "# StandardEncoder, StandardDecoder, VAE, Discriminator, old_vae_loss, discrim_loss\n",
        "# 若不在当前环境，请先粘贴我们之前整理过的“基础模型代码”。\n",
        "\n",
        "# ----------------------\n",
        "# 构建 Raman 侧 AAE 模型\n",
        "# ----------------------\n",
        "def build_raman_aae(input_dim, latent_dim=128, hidden_dim=2048, latent_norm='batch'):\n",
        "    \"\"\"\n",
        "    返回:\n",
        "      - raman_vae: VAE(encoder, decoder)，is_vae=False（与文献一致）\n",
        "      - discrim : 判别器，输出2个logits（source/target）\n",
        "    \"\"\"\n",
        "    raman_vae = VAE(\n",
        "        encoder=StandardEncoder(input_dim, latent_dim, hidden_dim=hidden_dim),\n",
        "        decoder=StandardDecoder(input_dim, latent_dim, hidden_dim=hidden_dim, no_final_relu=True),\n",
        "        is_vae=False,\n",
        "        latent_norm=latent_norm,\n",
        "    )\n",
        "    discrim = Discriminator(latent_dim=latent_dim, spectral=True, end_dim=2)\n",
        "    return raman_vae, discrim\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# 训练一步：判别器\n",
        "# ----------------------\n",
        "def d_step(discrim, ref_z, ram_z, opt_d, label_smooth=0.1):\n",
        "    discrim.train()\n",
        "    opt_d.zero_grad()\n",
        "\n",
        "    # one-hot + label smoothing\n",
        "    y_ref = torch.tensor([1.0 - label_smooth, 0.0 + label_smooth], device=ref_z.device)\n",
        "    y_ram = torch.tensor([0.0 + label_smooth, 1.0 - label_smooth], device=ref_z.device)\n",
        "    y = torch.cat([y_ref.expand(ref_z.size(0), 2),\n",
        "                   y_ram.expand(ram_z.size(0), 2)], dim=0)\n",
        "\n",
        "    logits = discrim(torch.cat([ref_z.detach(), ram_z.detach()], dim=0))\n",
        "    loss = F.binary_cross_entropy_with_logits(logits, y)\n",
        "    loss.backward()\n",
        "    opt_d.step()\n",
        "    return float(loss.item())\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# 训练一步：生成器（Raman VAE）\n",
        "# ----------------------\n",
        "def g_step(raman_vae, discrim, xb_raman, opt_g,\n",
        "          recon_weight=1.0, adv_weight=0.3,\n",
        "          aux_clf=None, y_raman=None, aux_weight=50.0):\n",
        "    raman_vae.train(); discrim.eval()\n",
        "    opt_g.zero_grad()\n",
        "\n",
        "    recon, mean, logvar, z = raman_vae(xb_raman)  # is_vae=False -> mean/logvar未用，仅接口保持\n",
        "    # 自重构损失（MSE）\n",
        "    recon_loss = F.mse_loss(recon, xb_raman)\n",
        "\n",
        "    # 对抗损失：让 Raman latent 被判为“source”(=ref)\n",
        "    target = torch.tensor([1.0, 0.0], device=xb_raman.device).expand(z.size(0), 2)\n",
        "    adv_loss = F.binary_cross_entropy_with_logits(discrim(z), target)\n",
        "\n",
        "    loss = recon_weight * recon_loss + adv_weight * adv_loss\n",
        "\n",
        "    # 可选：细胞类型辅助损失（把 ref 上训练好的分类器接到 Raman latent）\n",
        "    aux = None\n",
        "    if (aux_clf is not None) and (y_raman is not None):\n",
        "        aux_clf.eval()\n",
        "        logits_ct = aux_clf(z)\n",
        "        aux = F.cross_entropy(logits_ct, y_raman)\n",
        "        loss = loss + aux_weight * aux\n",
        "\n",
        "    loss.backward()\n",
        "    opt_g.step()\n",
        "    return {\n",
        "        \"recon\": float(recon_loss.item()),\n",
        "        \"adv\": float(adv_loss.item()),\n",
        "        \"aux\": (float(aux.item()) if aux is not None else None),\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 主函数：训练 Raman-AAF，把 Raman 对齐到 ref_vae 的潜空间\n",
        "# -------------------------------------------------\n",
        "def train_raman_aae(\n",
        "    *,\n",
        "    ref_vae,            # 已训练好的参考侧 AE/VAE（RNA 或 ATAC）\n",
        "    X_ref,              # 参考模态矩阵 (N_ref, F_ref) —— 只用于提供 ref latent\n",
        "    X_raman,            # Raman 矩阵 (N_raman, n_bins)\n",
        "    y_raman=None,       # 可选：Raman 的 cell_type int 标签 (N_raman,)\n",
        "    batch_size=64,\n",
        "    epochs=50,\n",
        "    latent_dim=128,\n",
        "    hidden_dim=2048,\n",
        "    lr_g=2e-4,\n",
        "    lr_d=5e-5,\n",
        "    recon_weight=1.0,\n",
        "    adv_weight=0.3,\n",
        "    aux_weight=50.0,\n",
        "    label_smooth=0.1,\n",
        "    use_latent_norm='batch',   # 'batch' or 'none'\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    device=None,\n",
        "    pretrained_aux_clf=None,   # 可选：在 ref latent 上训好的分类器（冻结）\n",
        "):\n",
        "    \"\"\"\n",
        "    训练完成后返回：\n",
        "      raman_vae, discrim, transfer_vae（encoder=raman, decoder=ref），训练日志 logs\n",
        "    \"\"\"\n",
        "    device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "    ref_vae = ref_vae.to(device).eval()\n",
        "    for p in ref_vae.parameters(): p.requires_grad = False\n",
        "\n",
        "    # DataLoader\n",
        "    if y_raman is None:\n",
        "        ram_ds = TensorDataset(torch.from_numpy(X_raman).float())\n",
        "    else:\n",
        "        ram_ds = TensorDataset(torch.from_numpy(X_raman).float(),\n",
        "                               torch.from_numpy(y_raman).long())\n",
        "    ref_ds = TensorDataset(torch.from_numpy(X_ref).float())\n",
        "\n",
        "    ram_loader = DataLoader(ram_ds, batch_size=batch_size, shuffle=True,\n",
        "                            num_workers=num_workers, pin_memory=pin_memory)\n",
        "    ref_loader = DataLoader(ref_ds, batch_size=batch_size, shuffle=True,\n",
        "                            num_workers=num_workers, pin_memory=pin_memory)\n",
        "\n",
        "    # 模型 & 优化器\n",
        "    input_dim = X_raman.shape[1]\n",
        "    raman_vae, discrim = build_raman_aae(input_dim, latent_dim, hidden_dim, latent_norm=use_latent_norm)\n",
        "    raman_vae = raman_vae.to(device)\n",
        "    discrim   = discrim.to(device)\n",
        "\n",
        "    opt_g = torch.optim.Adam(raman_vae.parameters(), lr=lr_g)\n",
        "    opt_d = torch.optim.Adam(discrim.parameters(),   lr=lr_d)\n",
        "\n",
        "    # 可选：辅助分类头（冻结，不训练）\n",
        "    aux_clf = None\n",
        "    if pretrained_aux_clf is not None:\n",
        "        aux_clf = pretrained_aux_clf.to(device)\n",
        "        for p in aux_clf.parameters(): p.requires_grad = False\n",
        "\n",
        "    print(\"Begin Raman AAE training ...\")\n",
        "    logs = []\n",
        "    for ep in range(1, epochs+1):\n",
        "        ep_d, ep_r, ep_a, ep_aux, nb = 0.0, 0.0, 0.0, 0.0, 0\n",
        "        for batch_r, batch_ref in zip(ram_loader, cycle(ref_loader)):\n",
        "            if y_raman is None:\n",
        "                (xb_r,) = batch_r\n",
        "                yb = None\n",
        "            else:\n",
        "                xb_r, yb = batch_r\n",
        "                yb = yb.to(device)\n",
        "\n",
        "            xb_r  = xb_r.to(device)\n",
        "            (xb_ref,) = batch_ref\n",
        "            xb_ref = xb_ref.to(device)\n",
        "\n",
        "            # 参考 latent（不回传梯度）\n",
        "            with torch.no_grad():\n",
        "                z_ref = ref_vae.get_latent(xb_ref)\n",
        "\n",
        "            # Raman latent\n",
        "            with torch.no_grad():\n",
        "                _, _, _, z_ram = raman_vae(xb_r)\n",
        "\n",
        "            # --- 判别器步 ---\n",
        "            dloss = d_step(discrim, z_ref, z_ram, opt_d, label_smooth=label_smooth)\n",
        "\n",
        "            # --- 生成器步（更新 Raman VAE）---\n",
        "            gout = g_step(raman_vae, discrim, xb_r, opt_g,\n",
        "                          recon_weight=recon_weight, adv_weight=adv_weight,\n",
        "                          aux_clf=aux_clf, y_raman=yb, aux_weight=aux_weight)\n",
        "\n",
        "            # 记录\n",
        "            ep_d   += dloss\n",
        "            ep_r   += gout[\"recon\"]\n",
        "            ep_a   += gout[\"adv\"]\n",
        "            ep_aux += (gout[\"aux\"] if gout[\"aux\"] is not None else 0.0)\n",
        "            nb     += 1\n",
        "\n",
        "        msg = f\"[{ep:03d}] D:{ep_d/nb:.4f}  Recon:{ep_r/nb:.5f}  Adv:{ep_a/nb:.5f}\"\n",
        "        if y_raman is not None and aux_clf is not None:\n",
        "            msg += f\"  Aux:{ep_aux/nb:.5f}\"\n",
        "        print(msg)\n",
        "        logs.append({\"epoch\": ep, \"D\": ep_d/nb, \"Recon\": ep_r/nb, \"Adv\": ep_a/nb,\n",
        "                     \"Aux\": (ep_aux/nb if (y_raman is not None and aux_clf is not None) else None)})\n",
        "\n",
        "    # 训练完构建 transfer_vae：encoder=raman, decoder=ref\n",
        "    transfer_vae = VAE(\n",
        "        encoder=raman_vae.encoder,      # 共享已训练的 Raman 编码器\n",
        "        decoder=ref_vae.decoder,        # 共享参考侧解码器\n",
        "        is_vae=False,\n",
        "        latent_norm=use_latent_norm,\n",
        "    ).to(device).eval()\n",
        "\n",
        "    return raman_vae.eval(), discrim.eval(), transfer_vae, logs\n"
      ],
      "metadata": {
        "id": "8koWtUgcIyEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 参考侧：RNA\n",
        "ref_vae = rna_ref_vae\n",
        "X_ref   = Xn_train  # 与训练 ref_vae 时使用的同一预处理\n",
        "# Raman\n",
        "X_raman = Xr        # 你的 Raman 矩阵\n",
        "y_raman = yr        # 可选：Raman 的 cell_type int 标签（若要用辅助损失）\n",
        "\n",
        "raman_vae, D, transfer_raman2rna, logs = train_raman_aae(\n",
        "    ref_vae=ref_vae,\n",
        "    X_ref=X_ref,\n",
        "    X_raman=X_raman,\n",
        "    y_raman=y_raman,                 # 若不用辅助损失可设 None\n",
        "    batch_size=64,\n",
        "    epochs=50,\n",
        "    latent_dim=128,\n",
        "    hidden_dim=2048,\n",
        "    lr_g=2e-4, lr_d=5e-5,\n",
        "    recon_weight=1.0, adv_weight=0.3,\n",
        "    aux_weight=50.0,\n",
        "    label_smooth=0.1,\n",
        "    use_latent_norm='batch',\n",
        "    pretrained_aux_clf=None          # 若有在RNA latent训练好的分类器就传进来\n",
        ")\n"
      ],
      "metadata": {
        "id": "4QvIpgXNI0GX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_vae = atac_ref_vae\n",
        "X_ref   = Xa_train\n",
        "X_raman = Xr\n",
        "y_raman = yr\n",
        "\n",
        "raman_vae_A, D_A, transfer_raman2atac, logs_A = train_raman_aae(\n",
        "    ref_vae=ref_vae,\n",
        "    X_ref=X_ref,\n",
        "    X_raman=X_raman,\n",
        "    y_raman=None,           # 没用辅助损失就设 None\n",
        "    epochs=50\n",
        ")\n"
      ],
      "metadata": {
        "id": "3gvaQsxQI0_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    xb = torch.from_numpy(Xr).float().to(next(raman_vae.parameters()).device)\n",
        "    xr_hat, _, _, _ = raman_vae(xb)\n",
        "    recon_mse = F.mse_loss(xr_hat, xb).item()\n",
        "print(\"Raman recon MSE =\", recon_mse)\n"
      ],
      "metadata": {
        "id": "Bkr1vViMI3A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    xb = torch.from_numpy(Xr).float().to(next(transfer_raman2rna.parameters()).device)\n",
        "    x_ref_pred, _, _, _ = transfer_raman2rna(xb)   # 形状=(N_raman, F_ref)\n"
      ],
      "metadata": {
        "id": "rmp5SkVSI5uH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}