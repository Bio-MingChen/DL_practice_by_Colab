{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOY4746hRzwZG6AqY3Aqbyo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bio-MingChen/DL_practice_by_Colab/blob/main/pytorch_test20250803.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Hp_uPMgY0R7G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "E3P4foY1HvQ1",
        "outputId": "87adac26-b068-4944-b126-85e92670f7d9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n"
      ],
      "metadata": {
        "id": "25tBOsgP0pYN"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXkFacDc9RsB",
        "outputId": "98111404-59fa-4517-daf3-eb7a700a010a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXm3vfUT9wo0",
        "outputId": "516f774c-e9d1-4fc4-983f-1b8016570401"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_dataloader = DataLoader(training_data,batch_size=batch_size,shuffle=True)\n",
        "test_dataloader = DataLoader(test_data,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "87x1nvZG9SRL"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X,y in test_dataloader:\n",
        "  print(f\"Shape of X [N,C,H,W]: {X.shape}\")\n",
        "  print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDpHvLw4-Ptg",
        "outputId": "cca9288b-6b99-4ad1-ba1a-4f5cba839e7a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N,C,H,W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataloader))[1].shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsesEq3J-3PU",
        "outputId": "bfe924d6-1da0-4213-9fed-9345a9075e70"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "img = training_data[0][0]\n",
        "label = training_data[0][1]\n",
        "plt.imshow(img.squeeze(),cmap=\"gray\")\n",
        "plt.title(labels_map[label])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "ubNT-HuM-Y6x",
        "outputId": "c4a08572-f43c-44fe-ce3c-72c0cb8f25f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Ankle Boot')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img.squeeze().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5euiuitA-sz",
        "outputId": "38989308-0d23-481a-f5a1-f7150654c645"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img.squeeze().unsqueeze(0).shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLSqzb9PBEyh",
        "outputId": "601aaf54-9b0f-4324-ca97-6a325a51f3f6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在 Python 的 `numpy` 和 `PyTorch`（以及 `TensorFlow`）中，`squeeze()` 是一个非常常用的函数，它的作用是：\n",
        "\n",
        "> **移除形状中为 1 的维度（size为1的维度）**\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 基本用法\n",
        "\n",
        "### NumPy 中：\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([[[1], [2], [3]]])  # shape: (1, 3, 1)\n",
        "x_squeezed = np.squeeze(x)       # shape: (3,)\n",
        "\n",
        "print(x.shape)        # (1, 3, 1)\n",
        "print(x_squeezed.shape)  # (3,)\n",
        "```\n",
        "\n",
        "### PyTorch 中：\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "x = torch.zeros(1, 3, 1)\n",
        "x_squeezed = x.squeeze()\n",
        "\n",
        "print(x.shape)        # torch.Size([1, 3, 1])\n",
        "print(x_squeezed.shape)  # torch.Size([3])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 什么时候会用到 `squeeze()`？\n",
        "\n",
        "### 1. **去掉多余的维度方便计算**\n",
        "\n",
        "很多时候加载数据或某些操作后，张量会多出不必要的 `1` 维度，可能会影响后续的计算，比如：\n",
        "\n",
        "```python\n",
        "y = model(x)  # y shape: (batch_size, 1)\n",
        "loss = loss_fn(y.squeeze(), target)  # 将 shape (batch_size, 1) -> (batch_size,)\n",
        "```\n",
        "\n",
        "### 2. **匹配维度**\n",
        "\n",
        "在计算 loss 或进行张量拼接（concatenate）时，维度不一致会报错，常见的做法就是用 `.squeeze()` 或 `.unsqueeze()` 来调整。\n",
        "\n",
        "### 3. **输出单个值**\n",
        "\n",
        "例如：\n",
        "\n",
        "```python\n",
        "x = torch.tensor([[[42]]])\n",
        "print(x.squeeze())  # tensor(42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 还可以指定要去掉的维度\n",
        "\n",
        "```python\n",
        "x = torch.randn(1, 3, 1)\n",
        "x_squeeze_dim0 = x.squeeze(0)  # 去掉第0维\n",
        "x_squeeze_dim2 = x.squeeze(2)  # 去掉第2维\n",
        "```\n",
        "\n",
        "⚠️ 如果指定的维度不为1，会报错。\n",
        "\n",
        "---\n",
        "\n",
        "## 🚫 注意事项\n",
        "\n",
        "* 只会去掉**维度是1**的部分；\n",
        "* `.squeeze()` 是不改变原张量的，需要赋值或使用 inplace 版本（如 `x.squeeze_()`）；\n",
        "* 可能会不小心把 batch 维度（维度为 1）挤掉，导致后续模型处理出错，所以要小心在什么时候用。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句话：\n",
        "\n",
        "> `squeeze()` 就是在你不想要多余的 1 维时，把它“挤掉”，常用于模型输出、reshape、数据对齐等场景。\n",
        "\n",
        "如果你有某段代码想确认是否需要 `.squeeze()`，可以发给我，我来帮你判断。\n"
      ],
      "metadata": {
        "id": "wIBEG44oBl7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm6y6-MUJGyE",
        "outputId": "c5e1b4cd-3b3c-40b2-ab07-e013e92695ba"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "28*28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4zWH5nUKWSE",
        "outputId": "a35fce07-cfff-4024-8599-5a6bd953b145"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        nn.Linear(512,512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        nn.Linear(512,10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr9W8BQsJ1YI",
        "outputId": "55ad8ad1-eac0-4f30-c14c-3e8d3a8d98c2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.2, inplace=False)\n",
            "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): Dropout(p=0.2, inplace=False)\n",
            "    (8): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "na050sSLMX0i"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| 层             | 作用                             |\n",
        "| ------------- | ------------------------------ |\n",
        "| `Linear`      | 全连接层，产生线性变换                    |\n",
        "| `BatchNorm1d` | 对线性输出做标准化，**稳定训练过程**，减轻内部协变量偏移 |\n",
        "| `ReLU`        | 非线性激活，给模型引入非线性能力               |\n",
        "| `Dropout`     | 在训练时随机丢弃部分激活值，**防止过拟合**        |\n"
      ],
      "metadata": {
        "id": "1x3aN1cVLQay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "当然可以，下面是用**通俗易懂的方式**解释 `BatchNorm1d` 的作用：\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 一句话总结：\n",
        "\n",
        "> **BatchNorm1d 就像是在每一层神经网络“做标准化处理”，帮你把数据“整理整齐”，让网络更容易学得快、学得稳、不容易崩。**\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 为什么要用 BatchNorm？\n",
        "\n",
        "想象你在教一个学生（神经网络）学习做题（训练任务）：\n",
        "\n",
        "* 如果题目（输入）格式很混乱，有的大有的小，有的全是负数，有的突然变大了；\n",
        "* 学生就会很难适应，学习过程会很不稳定，容易学崩、学慢。\n",
        "\n",
        "**BatchNorm 的作用就是把这些题目统一格式**：\n",
        "\n",
        "> 让数据变得“平均值是 0，标准差是 1”（标准化）\n",
        "\n",
        "这样学生就能**快速进入状态，集中精力做题（学习）**，不会因为题目形式变化而“心态崩了”。\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 BatchNorm1d 在干啥？\n",
        "\n",
        "以一句代码为例：\n",
        "\n",
        "```python\n",
        "nn.BatchNorm1d(512)\n",
        "```\n",
        "\n",
        "这个意思是说：对每个 batch 里的样本，在 `512` 维度上分别做如下操作：\n",
        "\n",
        "### 每一维特征都会：\n",
        "\n",
        "1. **计算当前 batch 的均值和方差**\n",
        "2. **把这维的数据标准化** → 减去均值 / 除以标准差\n",
        "3. **再乘一个“可学习的缩放因子” γ，加一个“偏移量” β**\n",
        "\n",
        "最终输出的是一个**标准化后还带有一定灵活性**的特征。\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 举个例子来形象理解：\n",
        "\n",
        "假设你喂给神经网络的数据是这样的：\n",
        "\n",
        "```python\n",
        "[[-100, 0, 300],\n",
        " [ -90, 1, 280],\n",
        " [-110, -1, 310]]\n",
        "```\n",
        "\n",
        "这个差异非常大，模型难学。\n",
        "\n",
        "BatchNorm 就会帮你把每一列**缩放和移动**，变成像这样：\n",
        "\n",
        "```python\n",
        "[[ 0.1, -0.2,  0.5],\n",
        " [-0.1,  0.0,  0.3],\n",
        " [-0.2,  0.2, -0.8]]\n",
        "```\n",
        "\n",
        "现在这些值就“整齐”多了，模型更容易处理。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 优点总结：\n",
        "\n",
        "| 优点        | 类比                  |\n",
        "| --------- | ------------------- |\n",
        "| 训练更快      | 学生面对统一格式的题，刷题效率提升   |\n",
        "| 训练更稳定     | 不容易“梯度爆炸/消失”        |\n",
        "| 减少对初始化的依赖 | 不用太担心初始权重选不好        |\n",
        "| 有轻微正则化效果  | 有点像 Dropout，能稍微防过拟合 |\n",
        "\n",
        "---\n",
        "\n",
        "## 🤔 为什么叫 `BatchNorm1d`？\n",
        "\n",
        "因为它是在**每一个 batch 上做“1D 的特征标准化”**（通常用于 `Linear` 层的输出），所以叫 `1d`。\n",
        "如果是 `Conv2d` 那种图像数据，就要用 `BatchNorm2d`。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句通俗话：\n",
        "\n",
        "> **BatchNorm1d 就是帮你的网络“整顿输入秩序”，标准化每一层的数据，让学习更高效、更稳定，不容易崩。**\n",
        "\n",
        "需要我用图示或动画形式解释，也可以画图给你看。\n"
      ],
      "metadata": {
        "id": "wg_K-MtoLq8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "非常好！你提到了几个深度学习中最关键的部分：**ReLU 激活函数、交叉熵损失函数 CrossEntropyLoss，以及 SGD 优化器**。我会一一给出它们的**公式、图像和作用解释**。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 一、ReLU（Rectified Linear Unit）激活函数\n",
        "\n",
        "### 🔸 公式：\n",
        "\n",
        "$$\n",
        "\\text{ReLU}(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "也就是说：\n",
        "\n",
        "* 如果 $x > 0$，输出就是 $x$\n",
        "* 如果 $x \\leq 0$，输出就是 0\n",
        "\n",
        "### 🔸 图像：\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = np.maximum(0, x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title('ReLU Activation Function')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('ReLU(x)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "（你可以运行上面代码看看图像，也可以我帮你画出图来）\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 作用：\n",
        "\n",
        "* 引入非线性，使神经网络可以学习复杂的函数；\n",
        "* 计算简单，训练快速；\n",
        "* 但可能出现“神经元死亡”问题（ReLU 输出长期为0）。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 二、`nn.CrossEntropyLoss()`\n",
        "\n",
        "### 🔸 用途：\n",
        "\n",
        "这是 **分类任务**中最常用的损失函数，适用于 `model` 的输出是 **logits**（未经过 `softmax` 的得分）。\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 计算过程分为两步（PyTorch 内部自动做的）：\n",
        "\n",
        "1. **先对 logits 做 softmax 得到预测概率：**\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
        "$$\n",
        "\n",
        "其中：\n",
        "\n",
        "* $z_i$：模型输出的第 $i$ 类的 logit\n",
        "* $\\hat{y}_i$：softmax 后的概率\n",
        "\n",
        "2. **再计算交叉熵损失（Cross Entropy）：**\n",
        "\n",
        "$$\n",
        "\\text{Loss} = -\\sum_{i} y_i \\log(\\hat{y}_i)\n",
        "$$\n",
        "\n",
        "其中：\n",
        "\n",
        "* $y_i$ 是 one-hot 标签（只有一个 1 其他都是 0）\n",
        "* 实际计算时只保留正确标签那一项：$-\\log(\\hat{y}_{\\text{true class}})$\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 举个例子：\n",
        "\n",
        "如果你的模型输出是：\n",
        "\n",
        "```python\n",
        "logits = [2.0, 1.0, 0.1]  # 表示三个类别的得分\n",
        "label = 0  # 正确类别是第0类\n",
        "```\n",
        "\n",
        "PyTorch 自动计算：\n",
        "\n",
        "```python\n",
        "softmax = [0.659, 0.242, 0.099]\n",
        "loss = -log(0.659) ≈ 0.417\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 三、`torch.optim.SGD(...)` 随机梯度下降优化器\n",
        "\n",
        "### 🔸 公式：\n",
        "\n",
        "每一轮迭代中，更新参数的公式是：\n",
        "\n",
        "$$\n",
        "\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} \\mathcal{L}\n",
        "$$\n",
        "\n",
        "其中：\n",
        "\n",
        "* $\\theta$：模型参数\n",
        "* $\\eta$：学习率（如 `1e-3`）\n",
        "* $\\nabla_{\\theta} \\mathcal{L}$：损失函数对参数的梯度\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 你写的代码：\n",
        "\n",
        "```python\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "```\n",
        "\n",
        "意思是：\n",
        "\n",
        "* 用 **交叉熵** 来评估分类误差；\n",
        "* 用 **学习率为 0.001 的 SGD 优化器** 来更新模型参数；\n",
        "* 每个 batch 后执行：\n",
        "\n",
        "```python\n",
        "loss.backward()        # 计算梯度\n",
        "optimizer.step()       # 根据梯度更新参数\n",
        "optimizer.zero_grad()  # 清空上一次的梯度\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结表格：\n",
        "\n",
        "| 项目                   | 公式                                                       | 用途         |\n",
        "| -------------------- | -------------------------------------------------------- | ---------- |\n",
        "| **ReLU**             | $\\max(0, x)$                                             | 激活函数，增加非线性 |\n",
        "| **CrossEntropyLoss** | $-\\log(\\hat{y}_{\\text{true class}})$                     | 分类任务损失     |\n",
        "| **SGD**              | $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\mathcal{L}$ | 梯度下降优化参数   |\n",
        "\n",
        "---\n",
        "\n",
        "如果你希望我把 ReLU 和 Softmax 的图画出来展示，也可以告诉我，我可以直接输出图片给你看。是否还想继续了解 Adam 优化器、学习率调度器之类的内容？\n"
      ],
      "metadata": {
        "id": "-q4Z2214NaEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "很好！这两个概念非常关键：**logits** 和 **softmax** 是在分类神经网络中经常遇到的术语。\n",
        "\n",
        "我来用通俗易懂的方式给你讲清楚它们的区别和作用：\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 一、什么是 **logits**？\n",
        "\n",
        "### 🔹一句话定义：\n",
        "\n",
        "> **logits 就是神经网络最后一层的原始输出分数**，还没有经过归一化，也不是概率。\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹举个例子：\n",
        "\n",
        "假设你要做一个 3 分类问题（比如识别猫🐱、狗🐶、鸟🐦）\n",
        "你喂一张图给模型，它最后输出：\n",
        "\n",
        "```python\n",
        "logits = [2.5, 0.3, -1.2]\n",
        "```\n",
        "\n",
        "这就是 logits：\n",
        "\n",
        "* 原始分数\n",
        "* 可以是正的、负的、任意大的\n",
        "* **不满足概率分布**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 二、什么是 **softmax**？\n",
        "\n",
        "### 🔹一句话定义：\n",
        "\n",
        "> **softmax 会把 logits 转换成“概率分布”**，每一类的得分变成了 0\\~1 之间的概率，所有概率加起来是 1。\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹公式：\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
        "$$\n",
        "\n",
        "其中：\n",
        "\n",
        "* $z_i$：logits 中第 i 类的分数\n",
        "* $\\hat{y}_i$：softmax 输出的概率\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹对前面例子的处理：\n",
        "\n",
        "```python\n",
        "logits = [2.5, 0.3, -1.2]\n",
        "```\n",
        "\n",
        "经过 softmax 后：\n",
        "\n",
        "```python\n",
        "softmax_probs = [0.81, 0.15, 0.04]  # 这些就是概率\n",
        "```\n",
        "\n",
        "这意味着：\n",
        "\n",
        "* 模型认为这张图是“猫”的概率是 81%\n",
        "* 是狗的概率是 15%\n",
        "* 是鸟的概率是 4%\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 logits vs softmax：比较总结\n",
        "\n",
        "| 比较项    | logits                  | softmax 结果            |\n",
        "| ------ | ----------------------- | --------------------- |\n",
        "| 值范围    | 任意实数                    | (0, 1) 之间             |\n",
        "| 是否是概率  | ❌ 不是                    | ✅ 是                   |\n",
        "| 是否归一化  | ❌ 否                     | ✅ 是（总和 = 1）           |\n",
        "| 模型输出格式 | 模型最后一层输出                | softmax 是对 logits 的处理 |\n",
        "| 用于哪    | 供 `CrossEntropyLoss` 使用 | 用于推理 / 可视化分类概率        |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ CrossEntropyLoss 自动帮你做了 softmax！\n",
        "\n",
        "你不需要手动 `softmax(logits)`，因为：\n",
        "\n",
        "```python\n",
        "nn.CrossEntropyLoss()\n",
        "```\n",
        "\n",
        "这个函数内部已经**自动对 logits 做了 softmax 计算**，然后才算 loss。\n",
        "\n",
        "所以你只需要把 `logits` 输出给它即可。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句话：\n",
        "\n",
        "> **logits 是模型输出的“原始分数”，softmax 是把这些分数转成概率的函数。**\n",
        "\n",
        "如果你想看 softmax 的图像、或者想自己实现 softmax，我可以也帮你画图或给代码。需要的话说一声！\n"
      ],
      "metadata": {
        "id": "UJaDlfNpNlyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这是一个非常经典的问题！💡**SGD（随机梯度下降）** 和 **Adam（自适应矩估计）** 是两种常用的深度学习优化器，它们的目标都是：\n",
        "\n",
        "> **通过不断更新参数，让模型损失最小化**\n",
        "\n",
        "但它们**更新参数的方式不同**，各有优缺点。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 一张表先概括它们的区别：\n",
        "\n",
        "| 特性         | SGD                  | Adam                        |\n",
        "| ---------- | -------------------- | --------------------------- |\n",
        "| 是否自适应学习率   | ❌ 否                  | ✅ 是（每个参数都有自己的学习率）           |\n",
        "| 是否用动量      | ✅ 可选（SGD + Momentum） | ✅ 自带                        |\n",
        "| 依赖过去梯度的平均值 | ❌ 否                  | ✅ 是（1阶和2阶矩估计）               |\n",
        "| 超参数敏感性     | 较高                   | 较低                          |\n",
        "| 收敛速度       | 慢（但稳）                | 快                           |\n",
        "| 是否容易过拟合    | 较不容易                 | 有时会更容易过拟合                   |\n",
        "| 适合         | 简单任务、大数据、容易泛化        | 稀疏梯度、复杂结构（如RNN、Transformer） |\n",
        "| 典型使用场景     | 经典CNN训练、简洁任务         | NLP、Transformer、预训练模型       |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 一、SGD（Stochastic Gradient Descent）\n",
        "\n",
        "### 📘 公式：\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(\\theta_t)\n",
        "$$\n",
        "\n",
        "* $\\theta$：模型参数\n",
        "* $\\eta$：学习率\n",
        "* $\\nabla_\\theta \\mathcal{L}$：损失对参数的梯度\n",
        "\n",
        "### 🧠 特点：\n",
        "\n",
        "* 每次用一个小批量（batch）数据计算梯度 → 更快迭代；\n",
        "* 容易陷入局部最小值；\n",
        "* 可加 momentum 增强稳定性：\n",
        "\n",
        "$$\n",
        "v_{t+1} = \\mu v_t - \\eta \\cdot \\nabla_\\theta \\mathcal{L} \\\\\n",
        "\\theta_{t+1} = \\theta_t + v_{t+1}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 二、Adam（Adaptive Moment Estimation）\n",
        "\n",
        "### 📘 公式概要：\n",
        "\n",
        "Adam 结合了：\n",
        "\n",
        "* **Momentum**（一阶矩估计，梯度的指数加权平均）；\n",
        "* **RMSProp**（二阶矩估计，平方梯度的平均）；\n",
        "\n",
        "$$\n",
        "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\quad \\text{(一阶动量)} \\\\\n",
        "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\quad \\text{(二阶动量)} \\\\\n",
        "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\quad \\text{(偏差修正)} \\\\\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "$$\n",
        "\n",
        "### 🧠 特点：\n",
        "\n",
        "* 学习率 **自动适应每个参数**；\n",
        "* 非常适合稀疏梯度的任务（如 NLP 中的词向量训练）；\n",
        "* 初始化不敏感，训练更稳定；\n",
        "* 收敛快，但可能泛化能力差（容易过拟合）；\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 简单理解：\n",
        "\n",
        "| 类比   | SGD           | Adam                  |\n",
        "| ---- | ------------- | --------------------- |\n",
        "| 你走迷宫 | 拿一张纸（当前梯度）照着走 | 手里拿指南针（过去方向）和地图（梯度分布） |\n",
        "| 走法   | 方向直来直去，可能被困   | 会绕路，会加速，会避免重复走错路      |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 实践建议\n",
        "\n",
        "* 🔹 **默认用 Adam**，如果你不确定或者任务很复杂；\n",
        "* 🔹 想要更好的**泛化能力**，可以考虑 **SGD + Momentum**；\n",
        "* 🔹 在预训练阶段用 Adam，在微调阶段换成 SGD 也很常见。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句话：\n",
        "\n",
        "> **Adam 是“聪明的自动驾驶”，跑得快也稳；SGD 是“手动挡老司机”，慢但容易控制，容易泛化。**\n",
        "\n",
        "---\n",
        "\n",
        "如果你想了解它们在具体项目中的表现（比如训练 ResNet、BERT 等模型时的表现），我可以继续补充对比案例。是否需要我演示一段两者对比训练的代码？\n"
      ],
      "metadata": {
        "id": "U_DLN-N9ORjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "非常好！`CrossEntropyLoss` 是分类问题中最常见的损失函数。它结合了 **Softmax + 负对数似然（Negative Log Likelihood）** 两个步骤，适用于**多分类**任务。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 一、CrossEntropyLoss 的计算公式\n",
        "\n",
        "对于一个样本的损失（假设类别总数是 $C$，模型输出 logits 是 $\\mathbf{z} = [z_1, z_2, ..., z_C]$，正确标签是第 $y$ 类）：\n",
        "\n",
        "1. **先做 softmax 得到概率分布**：\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\n",
        "$$\n",
        "\n",
        "2. **再对真实类别 $y$ 计算负对数概率**（交叉熵）：\n",
        "\n",
        "$$\n",
        "\\text{Loss} = -\\log(\\hat{y}_y)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 二、实际例子讲解（手动计算）\n",
        "\n",
        "假设你有 3 个类别，模型输出 logits：\n",
        "\n",
        "```python\n",
        "logits = [2.0, 1.0, 0.1]\n",
        "label = 0  # 正确类别是第 0 类\n",
        "```\n",
        "\n",
        "### 第一步：softmax\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "e^{2.0} &= 7.389 \\\\\n",
        "e^{1.0} &= 2.718 \\\\\n",
        "e^{0.1} &= 1.105 \\\\\n",
        "\\text{总和} &= 7.389 + 2.718 + 1.105 = 11.212 \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "得到 softmax 概率：\n",
        "\n",
        "```python\n",
        "[7.389/11.212, 2.718/11.212, 1.105/11.212] ≈ [0.659, 0.242, 0.099]\n",
        "```\n",
        "\n",
        "### 第二步：交叉熵损失\n",
        "\n",
        "由于正确标签是第 0 类（概率是 0.659）：\n",
        "\n",
        "$$\n",
        "\\text{Loss} = -\\log(0.659) ≈ 0.417\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 三、PyTorch 中的代码实现\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "logits = torch.tensor([[2.0, 1.0, 0.1]])  # shape: [batch_size=1, num_classes=3]\n",
        "labels = torch.tensor([0])  # 正确类别为 class 0\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss = loss_fn(logits, labels)\n",
        "\n",
        "print(loss.item())  # 输出大约 0.417\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ 注意事项\n",
        "\n",
        "* **输入 logits 不要先做 softmax**，PyTorch 的 `CrossEntropyLoss` 会**自动内部处理 softmax**；\n",
        "* `logits` 的 shape 是 `[batch_size, num_classes]`；\n",
        "* `labels` 是整型类别索引（不是 one-hot 向量），shape 是 `[batch_size]`；\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句话：\n",
        "\n",
        "> `CrossEntropyLoss = -log(正确类别的 softmax 概率)`，它度量模型“对正确答案的信心”，越不自信，损失越大。\n",
        "\n",
        "---\n",
        "\n",
        "是否还想看多个样本 batch 下的交叉熵计算？或者 one-hot 标签与交叉熵的关系？我可以继续讲。\n"
      ],
      "metadata": {
        "id": "w2DS0UeFOrdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataloader))\n",
        "len(train_dataloader.dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waSg2mmuO-uN",
        "outputId": "5a00dd38-e607-4e37-c882-a48b31b3b79f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # Compute prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xmnaerDUO1ml"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "非常棒的问题！你提到了两个深度学习训练过程中很关键但容易被忽略的细节：`model.train()` vs `model.eval()` 以及 test 中 loss 的计算。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 一、`model.train()` 和 `model.eval()` 是干嘛的？\n",
        "\n",
        "这两个方法是用于**切换模型的工作模式**的，它们不会直接影响 forward 运算的逻辑，但会影响**某些特定层的行为**，比如：\n",
        "\n",
        "| 模型层类型       | 在 `train()` 模式下行为  | 在 `eval()` 模式下行为 |\n",
        "| ----------- | ------------------ | ---------------- |\n",
        "| `Dropout`   | 每次随机丢掉一部分神经元       | 不进行 Dropout（即不丢） |\n",
        "| `BatchNorm` | 使用当前 batch 统计量做标准化 | 使用训练时保存的全局均值和方差  |\n",
        "\n",
        "### ✳️ 你代码中就有这些层：\n",
        "\n",
        "```python\n",
        "nn.Dropout(0.2),\n",
        "nn.BatchNorm1d(512),\n",
        "```\n",
        "\n",
        "所以：\n",
        "\n",
        "* 在训练时，使用 `model.train()` 是必须的，否则 Dropout 和 BatchNorm 不会生效；\n",
        "* 在测试/验证时，使用 `model.eval()` 是必须的，否则这些层的行为会导致结果不稳定或错误。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 二、关于 test 中 loss 的计算方式\n",
        "\n",
        "你说得没错，test 中每个 batch 的 loss 是不同的。但是代码这样处理：\n",
        "\n",
        "```python\n",
        "test_loss += loss_fn(pred, y).item()  # 累加每个 batch 的 loss\n",
        "...\n",
        "test_loss /= num_batches              # 最后除以 batch 数\n",
        "```\n",
        "\n",
        "这是在**计算所有 batch 的平均 loss（即平均每个 batch 的损失）**，这种写法虽然不是最严格的“平均每个样本的损失”，但**在 batch size 相同的情况下是等价的**。\n",
        "\n",
        "### 🧠 如果你想更严格，可以这样写：\n",
        "\n",
        "```python\n",
        "test_loss += loss_fn(pred, y).item() * X.size(0)  # 按照每个样本加权累加\n",
        "...\n",
        "test_loss /= size  # 最后除以总样本数\n",
        "```\n",
        "\n",
        "这种方式更精确，适合在 batch size 不一致时使用。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句话：\n",
        "\n",
        "* `model.train()`：设置为训练模式，**启用 Dropout 和 BatchNorm 训练行为**；\n",
        "* `model.eval()`：设置为测试模式，**关闭 Dropout，用 BatchNorm 的历史均值**；\n",
        "* 测试 loss 的写法没错，但在 batch size 不一致时建议加权平均。\n",
        "\n",
        "---\n",
        "\n",
        "如果你愿意，我可以帮你写一个完整版本的 `train` + `test` 框架，包括日志记录、早停、可视化等功能。是否需要？\n"
      ],
      "metadata": {
        "id": "JM676vmtT5yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这是一个非常关键的问题，很多初学者容易误解！你问的是：\n",
        "\n",
        "> `loss.item()` 是不是只取了第一个样本的 loss？\n",
        "\n",
        "### ✅ 答案是：**不是！`loss.item()` 是整个 batch 的平均 loss（标量）**\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 详细解释：\n",
        "\n",
        "你在代码中计算了：\n",
        "\n",
        "```python\n",
        "loss = loss_fn(pred, y)  # pred.shape: (batch_size, num_classes)\n",
        "```\n",
        "\n",
        "PyTorch 中默认的 `loss_fn = nn.CrossEntropyLoss()` **返回的是整个 batch 的平均 loss**，也就是说：\n",
        "\n",
        "$$\n",
        "\\text{loss} = \\frac{1}{N} \\sum_{i=1}^N \\text{CrossEntropy}(y_i, \\hat{y}_i)\n",
        "$$\n",
        "\n",
        "其中 $N$ 是当前 batch 的大小。\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 那 `loss.item()` 是做什么的？\n",
        "\n",
        "`loss` 是一个张量（`tensor(0.3874, grad_fn=...)`），它还带着计算图。\n",
        "\n",
        "> `loss.item()` 是把这个张量里的**单个标量值取出来**，变成 Python 的 float，方便你打印、记录或存入日志文件。\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 举个例子：\n",
        "\n",
        "假设你的 batch size 是 64，模型对这 64 个样本都做了预测，`loss_fn` 会返回：\n",
        "\n",
        "```python\n",
        "loss = tensor(0.3472, grad_fn=<...>)\n",
        "```\n",
        "\n",
        "你再执行：\n",
        "\n",
        "```python\n",
        "loss.item()  # → 0.3472（float 类型）\n",
        "```\n",
        "\n",
        "这个数值就是**这 64 个样本的平均 loss**，不是某一个样本的 loss。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 补充：想要每个样本的 loss 怎么做？\n",
        "\n",
        "可以用 `reduction='none'` 的方式：\n",
        "\n",
        "```python\n",
        "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
        "losses = loss_fn(pred, y)  # shape: (batch_size,)\n",
        "```\n",
        "\n",
        "此时：\n",
        "\n",
        "* `losses[i]` 就是第 `i` 个样本的 loss；\n",
        "* 如果你想自己算平均，可以：\n",
        "\n",
        "  ```python\n",
        "  loss = losses.mean()\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句话：\n",
        "\n",
        "> `loss.item()` 是整个 batch 的平均 loss（一个标量），**不是第一个样本的 loss**，它只是把 tensor 转为 float。\n",
        "\n",
        "如果你想要打印每个样本的损失，我可以帮你改一下代码展示。是否需要？\n"
      ],
      "metadata": {
        "id": "7bbDeJXMidY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n -------------------------------\")\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\n",
        "  test(test_dataloader, model, loss_fn)\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScdmarkNT4wm",
        "outputId": "9a52d02e-9c2b-4c96-9bef-cda36bc246aa"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            " -------------------------------\n",
            "loss: 2.351133 [   64/60000]\n",
            "loss: 1.504371 [ 6464/60000]\n",
            "loss: 1.231074 [12864/60000]\n",
            "loss: 1.036299 [19264/60000]\n",
            "loss: 0.953311 [25664/60000]\n",
            "loss: 0.944878 [32064/60000]\n",
            "loss: 0.852010 [38464/60000]\n",
            "loss: 0.914644 [44864/60000]\n",
            "loss: 0.760960 [51264/60000]\n",
            "loss: 0.678545 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.0%, Avg loss: 0.725412 \n",
            "\n",
            "Epoch 2\n",
            " -------------------------------\n",
            "loss: 0.797271 [   64/60000]\n",
            "loss: 0.630035 [ 6464/60000]\n",
            "loss: 0.684512 [12864/60000]\n",
            "loss: 0.679657 [19264/60000]\n",
            "loss: 0.832178 [25664/60000]\n",
            "loss: 0.604194 [32064/60000]\n",
            "loss: 0.648339 [38464/60000]\n",
            "loss: 0.796758 [44864/60000]\n",
            "loss: 0.723518 [51264/60000]\n",
            "loss: 0.660618 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.1%, Avg loss: 0.592749 \n",
            "\n",
            "Epoch 3\n",
            " -------------------------------\n",
            "loss: 0.732579 [   64/60000]\n",
            "loss: 0.669723 [ 6464/60000]\n",
            "loss: 0.673791 [12864/60000]\n",
            "loss: 0.489371 [19264/60000]\n",
            "loss: 0.478027 [25664/60000]\n",
            "loss: 0.461428 [32064/60000]\n",
            "loss: 0.515199 [38464/60000]\n",
            "loss: 0.667646 [44864/60000]\n",
            "loss: 0.694426 [51264/60000]\n",
            "loss: 0.561443 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.530315 \n",
            "\n",
            "Epoch 4\n",
            " -------------------------------\n",
            "loss: 0.626839 [   64/60000]\n",
            "loss: 0.796239 [ 6464/60000]\n",
            "loss: 0.576547 [12864/60000]\n",
            "loss: 0.452355 [19264/60000]\n",
            "loss: 0.582835 [25664/60000]\n",
            "loss: 0.502582 [32064/60000]\n",
            "loss: 0.560854 [38464/60000]\n",
            "loss: 0.438994 [44864/60000]\n",
            "loss: 0.451904 [51264/60000]\n",
            "loss: 0.360498 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.494021 \n",
            "\n",
            "Epoch 5\n",
            " -------------------------------\n",
            "loss: 0.480244 [   64/60000]\n",
            "loss: 0.508736 [ 6464/60000]\n",
            "loss: 0.450043 [12864/60000]\n",
            "loss: 0.537892 [19264/60000]\n",
            "loss: 0.412323 [25664/60000]\n",
            "loss: 0.416140 [32064/60000]\n",
            "loss: 0.386087 [38464/60000]\n",
            "loss: 0.685761 [44864/60000]\n",
            "loss: 0.311524 [51264/60000]\n",
            "loss: 0.407041 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.473002 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Save PyTorch Model State to model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0vu9mByVQFP",
        "outputId": "37e3070a-3189-483c-bbe5-b05b95589330"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save PyTorch Model State to model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading models\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
      ],
      "metadata": {
        "id": "vAeN37NFVaib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x,y = test_data[0]\n",
        "model.to(device)\n",
        "with torch.no_grad():\n",
        "  x =x.to(device)\n",
        "  pred = model(x)\n",
        "  print(pred)\n",
        "  # predicted = classes[pred[0].argmax(0)]\n",
        "  predicted = classes[pred.argmax(1).item()]\n",
        "  actual = classes[y]\n",
        "  print(f\"Predicted: {predicted}, Actual: {actual}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk0k-lL5XPTK",
        "outputId": "fd3198f6-bf9a-4998-e681-6e0a57c60016"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.7596, -1.9728, -1.8500, -1.4932, -2.2793,  2.3234, -1.8076,  3.5825,\n",
            "         -0.0408,  4.3602]], device='cuda:0')\n",
            "Predicted: Ankle boot, Actual: Ankle boot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "很好！让我们逐步拆解 `pred(x)[0].argmax(0)` 的含义，它经常出现在**分类模型预测中**。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 假设背景：\n",
        "\n",
        "你有一个**多分类模型**（比如用于识别手写数字的 MNIST），模型输入一个样本后输出一个向量（logits），例如：\n",
        "\n",
        "```python\n",
        "pred(x)  → tensor([[0.2, -1.0, 2.3, 0.5, ..., -0.6]])  # shape: [1, num_classes]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 逐步解释 `pred(x)[0].argmax(0)`\n",
        "\n",
        "### 🔹 `pred(x)`\n",
        "\n",
        "模型对输入 `x` 的预测输出，形状一般是 `[1, num_classes]`（1 表示 batch size = 1）\n",
        "\n",
        "例如：\n",
        "\n",
        "```python\n",
        "tensor([[0.2, -1.0, 2.3, 0.5, -0.6]])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 `pred(x)[0]`\n",
        "\n",
        "取出第一个（也是唯一一个）样本的预测结果，变成一维 tensor：\n",
        "\n",
        "```python\n",
        "tensor([0.2, -1.0, 2.3, 0.5, -0.6])  # shape: [num_classes]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 `.argmax(0)`\n",
        "\n",
        "在维度 0（因为是一维向量）上取最大值的**索引**（index）：\n",
        "\n",
        "```python\n",
        "argmax(0) → 2  # 因为 2.3 是最大值，它在第 2 个位置\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 最终意义：\n",
        "\n",
        "```python\n",
        "pred(x)[0].argmax(0)\n",
        "```\n",
        "\n",
        "这段代码的作用是：\n",
        "\n",
        "> **得到模型对样本 `x` 的预测类别（即分数最高的类别的索引）**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 示例总结：\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "output = torch.tensor([[0.2, -1.0, 2.3, 0.5, -0.6]])\n",
        "pred_class = output[0].argmax(0)\n",
        "print(pred_class)  # 输出：tensor(2)\n",
        "```\n",
        "\n",
        "代表这个样本的预测类别是类 2。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 扩展建议：\n",
        "\n",
        "如果你有多个样本，建议使用：\n",
        "\n",
        "```python\n",
        "pred_classes = pred(x).argmax(dim=1)\n",
        "```\n",
        "\n",
        "这样可以一次性得到 batch 内所有样本的预测类别。\n",
        "\n",
        "---\n",
        "\n",
        "是否需要我画一张图来说明 argmax 的含义？或者讲讲 `softmax + argmax` 和 `CrossEntropyLoss` 之间的关系？\n"
      ],
      "metadata": {
        "id": "aJ9etjwQX-ts"
      }
    }
  ]
}