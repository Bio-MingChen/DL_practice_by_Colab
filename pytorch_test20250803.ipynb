{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyODVksC6PWzl8unAWy7O9Qp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bio-MingChen/DL_practice_by_Colab/blob/main/pytorch_test20250803.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Hp_uPMgY0R7G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "E3P4foY1HvQ1",
        "outputId": "87adac26-b068-4944-b126-85e92670f7d9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n"
      ],
      "metadata": {
        "id": "25tBOsgP0pYN"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXkFacDc9RsB",
        "outputId": "98111404-59fa-4517-daf3-eb7a700a010a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXm3vfUT9wo0",
        "outputId": "516f774c-e9d1-4fc4-983f-1b8016570401"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_dataloader = DataLoader(training_data,batch_size=batch_size,shuffle=True)\n",
        "test_dataloader = DataLoader(test_data,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "87x1nvZG9SRL"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X,y in test_dataloader:\n",
        "  print(f\"Shape of X [N,C,H,W]: {X.shape}\")\n",
        "  print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDpHvLw4-Ptg",
        "outputId": "cca9288b-6b99-4ad1-ba1a-4f5cba839e7a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N,C,H,W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataloader))[1].shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsesEq3J-3PU",
        "outputId": "bfe924d6-1da0-4213-9fed-9345a9075e70"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "img = training_data[0][0]\n",
        "label = training_data[0][1]\n",
        "plt.imshow(img.squeeze(),cmap=\"gray\")\n",
        "plt.title(labels_map[label])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "ubNT-HuM-Y6x",
        "outputId": "c4a08572-f43c-44fe-ce3c-72c0cb8f25f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Ankle Boot')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKONJREFUeJzt3Xt0VeWdxvHnBJJDIBdMIDcJhKugXDqDEPCCIBQIiiKoeJlZ0LFSmVBFxtFhVivSzlppcaZl2aHQyyyw0yBCy6VSxYUgoQqIIAy61AghCAgJl5qTkBtJzjt/sDz1ECC82yRvEr6ftfbS7PP+st+87uRxn7PP7/iMMUYAADSzCNcTAABcmwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwgg4ApmzpypmJiYBseNHj1ao0ePbvoJAW0IAYQ255e//KV8Pp8yMzNdT8WzmTNnyufzhbb27dsrPT1dDz30kD7++OMmPXZFRYVeeOEFbdu2rUmPA7R3PQGgseXm5iojI0O7d+/WoUOH1KdPH9dT8sTv9+u3v/2tJKm2tlYFBQVatmyZNm3apI8//lhpaWlNctyKigotXLhQkriqQ5MigNCmFBYWaseOHVq7dq2+973vKTc3VwsWLHA9LU/at2+vf/iHfwjbN2LECN19993685//rMcff9zRzIDGwVNwaFNyc3N13XXX6a677tL999+v3NzcemOOHDkin8+n//zP/9Svf/1r9e7dW36/X8OGDdP777/f4DH279+vrl27avTo0Tp37txlx1VXV2vBggXq06eP/H6/0tPT9eyzz6q6utrzz5eSkiLpQjh93eHDh/XAAw8oISFBHTt21IgRI/TnP/+5Xv2pU6f02GOPKTk5WR06dNCQIUP08ssvhx4/cuSIunbtKklauHBh6CnAF154wfOcgcvhCghtSm5urqZOnaqoqCg9/PDDWrp0qd5//30NGzas3tiVK1eqrKxM3/ve9+Tz+bRo0SJNnTpVhw8fVmRk5CW///vvv68JEybo5ptv1oYNGxQdHX3JccFgUPfcc4/eeecdzZo1SwMGDNCHH36on//85/rss8+0fv36q/p5zpw5I0mqq6vT4cOH9dxzzykxMVF33313aExxcbFuueUWVVRU6Mknn1RiYqJefvll3XPPPfrDH/6g++67T5JUWVmp0aNH69ChQ5ozZ4569uypNWvWaObMmSopKdFTTz2lrl27aunSpZo9e7buu+8+TZ06VZI0ePDgq5ovYMUAbcSePXuMJLN582ZjjDHBYNB069bNPPXUU2HjCgsLjSSTmJho/vrXv4b2b9iwwUgyr732WmjfjBkzTKdOnYwxxrzzzjsmLi7O3HXXXaaqqirse95xxx3mjjvuCH39v//7vyYiIsL85S9/CRu3bNkyI8m8++67V/xZZsyYYSTV266//nqzd+/esLFz5841ksKOVVZWZnr27GkyMjJMXV2dMcaYxYsXG0nm97//fWjc+fPnzciRI01MTIwpLS01xhhz+vRpI8ksWLDginMEvimegkObkZubq+TkZI0ZM0aS5PP5NH36dK1atUp1dXX1xk+fPl3XXXdd6Ovbb79d0oWnsy729ttva8KECRo7dqzWrl0rv99/xbmsWbNGAwYMUP/+/XXmzJnQduedd4a+X0M6dOigzZs3a/PmzXrzzTf1q1/9SjExMZo0aZI+++yz0LjXX39dw4cP12233RbaFxMTo1mzZunIkSOhu+Zef/11paSk6OGHHw6Ni4yM1JNPPqlz584pLy+vwTkBjYmn4NAm1NXVadWqVRozZowKCwtD+zMzM/Vf//Vf2rJli8aPHx9W071797CvvwqjL7/8Mmx/VVWV7rrrLg0dOlSrV6+u9/rLpRw8eFCffPJJ6PWUi506darB79GuXTuNGzcubN+kSZPUt29fzZ8/X3/84x8lSZ9//vklbzkfMGBA6PGBAwfq888/V9++fRUREXHZcUBzIoDQJmzdulUnT57UqlWrtGrVqnqP5+bm1gugdu3aXfJ7mYs+pd7v92vSpEnasGGDNm3aFPb6y+UEg0ENGjRIP/vZzy75eHp6eoPf41K6deumG264Qdu3b/dUD7QkBBDahNzcXCUlJWnJkiX1Hlu7dq3WrVunZcuWXfamgSvx+XzKzc3VvffeqwceeEBvvPFGg++P6d27t/7v//5PY8eOlc/nsz7mldTW1obdfdejRw/l5+fXG/fpp5+GHv/qnwcOHFAwGAy7Crp4XGPPF7gcXgNCq1dZWam1a9fq7rvv1v33319vmzNnjsrKyvSnP/3J8zGioqK0du1aDRs2TJMnT9bu3buvOP7BBx/UF198od/85jeXnG95ebmneXz22WfKz8/XkCFDQvsmTZqk3bt3a+fOnaF95eXl+vWvf62MjAzdeOONoXFFRUV69dVXQ+Nqa2v1i1/8QjExMbrjjjskSR07dpQklZSUeJojcLW4AkKr96c//UllZWW65557Lvn4iBEj1LVrV+Xm5mr69OmejxMdHa2NGzfqzjvvVFZWlvLy8jRw4MBLjv3Hf/xHrV69Wk888YTefvtt3Xrrraqrq9Onn36q1atX680339TNN998xePV1tbq97//vaQLT+kdOXJEy5YtUzAYDHtz7b/927/plVdeUVZWlp588kklJCTo5ZdfVmFhof74xz+GrnZmzZqlX/3qV5o5c6b27t2rjIwM/eEPf9C7776rxYsXKzY2NvRz3njjjXr11VfVr18/JSQkaODAgZf9WQHPXN+GB3xTkydPNh06dDDl5eWXHTNz5kwTGRlpzpw5E7oN+8UXX6w3Thfdfvz127C/cubMGXPjjTealJQUc/DgQWNM/duwjblwi/NPf/pTc9NNNxm/32+uu+46M3ToULNw4UITCASu+DNd6jbsuLg4M3bsWPPWW2/VG19QUGDuv/9+07lzZ9OhQwczfPhws3HjxnrjiouLzXe+8x3TpUsXExUVZQYNGmSWL19eb9yOHTvM0KFDTVRUFLdko8n4jLnoFVcAAJoBrwEBAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOBEi3sjajAY1IkTJxQbG0tLEABohYwxKisrU1paWr3mt1/X4gLoxIkTnhs1AgBajmPHjqlbt26XfbzFPQX3VTsQAEDr1tDf8yYLoCVLligjI0MdOnRQZmZmg80bv8LTbgDQNjT097xJAujVV1/VvHnztGDBAn3wwQcaMmSIJkyYcFUfwgUAuEY0RYO54cOHm+zs7NDXdXV1Ji0tzeTk5DRYGwgE6jVhZGNjY2NrfVtDTXcb/Qro/Pnz2rt3b9hHCUdERGjcuHFhn1fylerqapWWloZtAIC2r9ED6MyZM6qrq1NycnLY/uTkZBUVFdUbn5OTo/j4+NDGHXAAcG1wfhfc/PnzFQgEQtuxY8dcTwkA0Awa/X1AXbp0Ubt27VRcXBy2v7i4WCkpKfXG+/1++f3+xp4GAKCFa/QroKioKA0dOlRbtmwJ7QsGg9qyZYtGjhzZ2IcDALRSTdIJYd68eZoxY4ZuvvlmDR8+XIsXL1Z5ebm+853vNMXhAACtUJME0PTp03X69Gk9//zzKioq0re+9S1t2rSp3o0JAIBrl88YY1xP4utKS0sVHx/vehoAgG8oEAgoLi7uso87vwsOAHBtIoAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE60dz0BoCXx+XzWNcaYJphJfbGxsdY1t912m6djvfHGG57qbHlZ73bt2lnX1NbWWte0dF7WzqumOse5AgIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJ2hGCnxNRIT9/5PV1dVZ1/Tp08e65rvf/a51TWVlpXWNJJWXl1vXVFVVWdfs3r3buqY5G4t6afjp5RzycpzmXAfbBrDGGAWDwQbHcQUEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE7QjBT4Gtumi5K3ZqR33nmndc24ceOsa44fP25dI0l+v9+6pmPHjtY13/72t61rfvvb31rXFBcXW9dIF5pq2vJyPngRExPjqe5qmoRerKKiwtOxGsIVEADACQIIAOBEowfQCy+8IJ/PF7b179+/sQ8DAGjlmuQ1oJtuuklvvfXW3w7SnpeaAADhmiQZ2rdvr5SUlKb41gCANqJJXgM6ePCg0tLS1KtXLz366KM6evToZcdWV1ertLQ0bAMAtH2NHkCZmZlasWKFNm3apKVLl6qwsFC33367ysrKLjk+JydH8fHxoS09Pb2xpwQAaIEaPYCysrL0wAMPaPDgwZowYYJef/11lZSUaPXq1ZccP3/+fAUCgdB27Nixxp4SAKAFavK7Azp37qx+/frp0KFDl3zc7/d7etMbAKB1a/L3AZ07d04FBQVKTU1t6kMBAFqRRg+gZ555Rnl5eTpy5Ih27Nih++67T+3atdPDDz/c2IcCALRijf4U3PHjx/Xwww/r7Nmz6tq1q2677Tbt2rVLXbt2bexDAQBasUYPoFWrVjX2twSazfnz55vlOMOGDbOuycjIsK7x0lxVkiIi7J8cefPNN61r/u7v/s66ZtGiRdY1e/bssa6RpA8//NC65pNPPrGuGT58uHWNl3NIknbs2GFds3PnTqvxxpireksNveAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwIkm/0A6wAWfz+epzhhjXfPtb3/buubmm2+2rrncx9pfSadOnaxrJKlfv37NUvP+++9b11zuwy2vJCYmxrpGkkaOHGldM3XqVOuampoa6xovaydJ3/3ud61rqqurrcbX1tbqL3/5S4PjuAICADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEz7jpf1vEyotLVV8fLzraaCJeO1S3Vy8/Drs2rXLuiYjI8O6xguv611bW2tdc/78eU/HslVVVWVdEwwGPR3rgw8+sK7x0q3by3pPnDjRukaSevXqZV1z/fXXezpWIBBQXFzcZR/nCggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnGjvegK4trSw3reN4ssvv7SuSU1Nta6prKy0rvH7/dY1ktS+vf2fhpiYGOsaL41Fo6OjrWu8NiO9/fbbrWtuueUW65qICPtrgaSkJOsaSdq0aZOnuqbAFRAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEzUuAb6tixo3WNl+aTXmoqKiqsayQpEAhY15w9e9a6JiMjw7rGS0Nbn89nXSN5W3Mv50NdXZ11jdcGq+np6Z7qmgJXQAAAJwggAIAT1gG0fft2TZ48WWlpafL5fFq/fn3Y48YYPf/880pNTVV0dLTGjRungwcPNtZ8AQBthHUAlZeXa8iQIVqyZMklH1+0aJFeeuklLVu2TO+99546deqkCRMmePrgKQBA22V9E0JWVpaysrIu+ZgxRosXL9YPfvAD3XvvvZKk3/3ud0pOTtb69ev10EMPfbPZAgDajEZ9DaiwsFBFRUUaN25caF98fLwyMzO1c+fOS9ZUV1ertLQ0bAMAtH2NGkBFRUWSpOTk5LD9ycnJocculpOTo/j4+NDWkm4RBAA0Hed3wc2fP1+BQCC0HTt2zPWUAADNoFEDKCUlRZJUXFwctr+4uDj02MX8fr/i4uLCNgBA29eoAdSzZ0+lpKRoy5YtoX2lpaV67733NHLkyMY8FACglbO+C+7cuXM6dOhQ6OvCwkLt379fCQkJ6t69u+bOnav/+I//UN++fdWzZ0/98Ic/VFpamqZMmdKY8wYAtHLWAbRnzx6NGTMm9PW8efMkSTNmzNCKFSv07LPPqry8XLNmzVJJSYluu+02bdq0SR06dGi8WQMAWj2f8dLZrwmVlpYqPj7e9TTQRLw0hfTSENJLc0dJiomJsa7Zt2+fdY2XdaisrLSu8fv91jWSdOLECeuai1/7vRq33HKLdY2XpqdeGoRKUlRUlHVNWVmZdY2Xv3leb9jyco4/9thjVuPr6uq0b98+BQKBK76u7/wuOADAtYkAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnrD+OAfgmvDRfb9eunXWN127Y06dPt6653Kf9Xsnp06eta6Kjo61rgsGgdY0kderUybomPT3duub8+fPWNV46fNfU1FjXSFL79vZ/Ir38d0pMTLSuWbJkiXWNJH3rW9+yrvGyDleDKyAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIJmpGhWXpoaemlY6dVHH31kXVNdXW1dExkZaV3TnE1Zk5KSrGuqqqqsa86ePWtd42XtOnToYF0jeWvK+uWXX1rXHD9+3LrmkUcesa6RpBdffNG6ZteuXZ6O1RCugAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADAiWu6GanP5/NU56UpZESEfdZ7mV9NTY11TTAYtK7xqra2ttmO5cXrr79uXVNeXm5dU1lZaV0TFRVlXWOMsa6RpNOnT1vXePm98NIk1Ms57lVz/T55WbvBgwdb10hSIBDwVNcUuAICADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACfaTDNSL8386urqPB2rpTfUbMlGjRplXTNt2jTrmltvvdW6RpIqKiqsa86ePWtd46WxaPv29r+uXs9xL+vg5XfQ7/db13hpYOq1KauXdfDCy/lw7tw5T8eaOnWqdc1rr73m6VgN4QoIAOAEAQQAcMI6gLZv367JkycrLS1NPp9P69evD3t85syZ8vl8YdvEiRMba74AgDbCOoDKy8s1ZMgQLVmy5LJjJk6cqJMnT4a2V1555RtNEgDQ9li/qpmVlaWsrKwrjvH7/UpJSfE8KQBA29ckrwFt27ZNSUlJuuGGGzR79uwr3iVUXV2t0tLSsA0A0PY1egBNnDhRv/vd77Rlyxb99Kc/VV5enrKysi57O2hOTo7i4+NDW3p6emNPCQDQAjX6+4Aeeuih0L8PGjRIgwcPVu/evbVt2zaNHTu23vj58+dr3rx5oa9LS0sJIQC4BjT5bdi9evVSly5ddOjQoUs+7vf7FRcXF7YBANq+Jg+g48eP6+zZs0pNTW3qQwEAWhHrp+DOnTsXdjVTWFio/fv3KyEhQQkJCVq4cKGmTZumlJQUFRQU6Nlnn1WfPn00YcKERp04AKB1sw6gPXv2aMyYMaGvv3r9ZsaMGVq6dKkOHDigl19+WSUlJUpLS9P48eP14x//2FPPJwBA2+UzXrv0NZHS0lLFx8e7nkajS0hIsK5JS0uzrunbt2+zHEfy1tSwX79+1jXV1dXWNRER3p5drqmpsa6Jjo62rjlx4oR1TWRkpHWNlyaXkpSYmGhdc/78eeuajh07Wtfs2LHDuiYmJsa6RvLWPDcYDFrXBAIB6xov54MkFRcXW9cMGDDA07ECgcAVX9enFxwAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcaPSP5HZlxIgR1jU//vGPPR2ra9eu1jWdO3e2rqmrq7OuadeunXVNSUmJdY0k1dbWWteUlZVZ13jpsuzz+axrJKmystK6xkt35gcffNC6Zs+ePdY1sbGx1jWStw7kGRkZno5la9CgQdY1Xtfh2LFj1jUVFRXWNV46qnvt8N2jRw9PdU2BKyAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcKLFNiONiIiwaij50ksvWR8jNTXVukby1iTUS42XpoZeREVFearz8jN5afbpRXx8vKc6L40af/KTn1jXeFmH2bNnW9ecOHHCukaSqqqqrGu2bNliXXP48GHrmr59+1rXJCYmWtdI3hrhRkZGWtdERNhfC9TU1FjXSNLp06c91TUFroAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAmfMca4nsTXlZaWKj4+Xo8++qhVk0wvDSELCgqsayQpJiamWWr8fr91jRdemidK3hp+Hjt2zLrGS0PNrl27WtdI3ppCpqSkWNdMmTLFuqZDhw7WNRkZGdY1krfzdejQoc1S4+W/kZemol6P5bW5ry2bZs1f5+X3fcSIEVbjg8GgvvjiCwUCAcXFxV12HFdAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOBEe9cTuJzTp09bNc3z0uQyNjbWukaSqqurrWu8zM9LQ0gvjRCv1CzwSv76179a13z++efWNV7WobKy0rpGkqqqqqxramtrrWvWrVtnXfPhhx9a13htRpqQkGBd46XhZ0lJiXVNTU2NdY2X/0bShaaatrw0+/RyHK/NSL38jejXr5/V+NraWn3xxRcNjuMKCADgBAEEAHDCKoBycnI0bNgwxcbGKikpSVOmTFF+fn7YmKqqKmVnZysxMVExMTGaNm2aiouLG3XSAIDWzyqA8vLylJ2drV27dmnz5s2qqanR+PHjVV5eHhrz9NNP67XXXtOaNWuUl5enEydOaOrUqY0+cQBA62Z1E8KmTZvCvl6xYoWSkpK0d+9ejRo1SoFAQP/zP/+jlStX6s4775QkLV++XAMGDNCuXbusP1UPANB2faPXgAKBgKS/3TGzd+9e1dTUaNy4caEx/fv3V/fu3bVz585Lfo/q6mqVlpaGbQCAts9zAAWDQc2dO1e33nqrBg4cKEkqKipSVFSUOnfuHDY2OTlZRUVFl/w+OTk5io+PD23p6elepwQAaEU8B1B2drY++ugjrVq16htNYP78+QoEAqHNy/tlAACtj6c3os6ZM0cbN27U9u3b1a1bt9D+lJQUnT9/XiUlJWFXQcXFxUpJSbnk9/L7/fL7/V6mAQBoxayugIwxmjNnjtatW6etW7eqZ8+eYY8PHTpUkZGR2rJlS2hffn6+jh49qpEjRzbOjAEAbYLVFVB2drZWrlypDRs2KDY2NvS6Tnx8vKKjoxUfH6/HHntM8+bNU0JCguLi4vT9739fI0eO5A44AEAYqwBaunSpJGn06NFh+5cvX66ZM2dKkn7+858rIiJC06ZNU3V1tSZMmKBf/vKXjTJZAEDb4TPGGNeT+LrS0lLFx8dr0KBBateu3VXX/eY3v7E+1pkzZ6xrJKlTp07WNYmJidY1Xho1njt3zrrGS/NESWrf3v4lRC9NFzt27Ghd46WBqeRtLSIi7O/l8fJrd/HdpVfj628St+GlmeuXX35pXePl9V8vv7deGphK3pqYejlWdHS0dc3lXldviJcmprm5uVbjq6ur9d///d8KBAJXbHZMLzgAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA44ekTUZvDhx9+aDV+7dq11sf4p3/6J+saSTpx4oR1zeHDh61rqqqqrGu8dIH22g3bSwffqKgo6xqbruhfqa6utq6RpLq6OusaL52tKyoqrGtOnjxpXeO12b2XdfDSHb25zvHz589b10jeOtJ7qfHSQdtLp25J9T5I9GoUFxdbjb/a9eYKCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCc8Bmv3QqbSGlpqeLj45vlWFlZWZ7qnnnmGeuapKQk65ozZ85Y13hphOil8aTkrUmol2akXppcepmbJPl8PusaL79CXhrAeqnxst5ej+Vl7bzwchzbZprfhJc1DwaD1jUpKSnWNZJ04MAB65oHH3zQ07ECgYDi4uIu+zhXQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgRIttRurz+ayaDnpp5tecxowZY12Tk5NjXeOl6anX5q8REfb//+KlSaiXZqReG6x6cerUKesaL792X3zxhXWN19+Lc+fOWdd4bQBry8va1dTUeDpWRUWFdY2X34vNmzdb13zyySfWNZK0Y8cOT3Ve0IwUANAiEUAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMCJFtuMFM2nf//+nuq6dOliXVNSUmJd061bN+uaI0eOWNdI3ppWFhQUeDoW0NbRjBQA0CIRQAAAJ6wCKCcnR8OGDVNsbKySkpI0ZcoU5efnh40ZPXp06LN8vtqeeOKJRp00AKD1swqgvLw8ZWdna9euXdq8ebNqamo0fvx4lZeXh417/PHHdfLkydC2aNGiRp00AKD1s/qoyU2bNoV9vWLFCiUlJWnv3r0aNWpUaH/Hjh2VkpLSODMEALRJ3+g1oEAgIElKSEgI25+bm6suXbpo4MCBmj9//hU/1ra6ulqlpaVhGwCg7bO6Avq6YDCouXPn6tZbb9XAgQND+x955BH16NFDaWlpOnDggJ577jnl5+dr7dq1l/w+OTk5WrhwoddpAABaKc/vA5o9e7beeOMNvfPOO1d8n8bWrVs1duxYHTp0SL179673eHV1taqrq0Nfl5aWKj093cuU4BHvA/ob3gcENJ6G3gfk6Qpozpw52rhxo7Zv397gH4fMzExJumwA+f1++f1+L9MAALRiVgFkjNH3v/99rVu3Ttu2bVPPnj0brNm/f78kKTU11dMEAQBtk1UAZWdna+XKldqwYYNiY2NVVFQkSYqPj1d0dLQKCgq0cuVKTZo0SYmJiTpw4ICefvppjRo1SoMHD26SHwAA0DpZBdDSpUslXXiz6dctX75cM2fOVFRUlN566y0tXrxY5eXlSk9P17Rp0/SDH/yg0SYMAGgbrJ+Cu5L09HTl5eV9owkBAK4NdMMGADQJumEDAFokAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEy0ugIwxrqcAAGgEDf09b3EBVFZW5noKAIBG0NDfc59pYZccwWBQJ06cUGxsrHw+X9hjpaWlSk9P17FjxxQXF+dohu6xDhewDhewDhewDhe0hHUwxqisrExpaWmKiLj8dU77ZpzTVYmIiFC3bt2uOCYuLu6aPsG+wjpcwDpcwDpcwDpc4Hod4uPjGxzT4p6CAwBcGwggAIATrSqA/H6/FixYIL/f73oqTrEOF7AOF7AOF7AOF7SmdWhxNyEAAK4NreoKCADQdhBAAAAnCCAAgBMEEADACQIIAOBEqwmgJUuWKCMjQx06dFBmZqZ2797tekrN7oUXXpDP5wvb+vfv73paTW779u2aPHmy0tLS5PP5tH79+rDHjTF6/vnnlZqaqujoaI0bN04HDx50M9km1NA6zJw5s975MXHiRDeTbSI5OTkaNmyYYmNjlZSUpClTpig/Pz9sTFVVlbKzs5WYmKiYmBhNmzZNxcXFjmbcNK5mHUaPHl3vfHjiiScczfjSWkUAvfrqq5o3b54WLFigDz74QEOGDNGECRN06tQp11NrdjfddJNOnjwZ2t555x3XU2py5eXlGjJkiJYsWXLJxxctWqSXXnpJy5Yt03vvvadOnTppwoQJqqqqauaZNq2G1kGSJk6cGHZ+vPLKK804w6aXl5en7Oxs7dq1S5s3b1ZNTY3Gjx+v8vLy0Jinn35ar732mtasWaO8vDydOHFCU6dOdTjrxnc16yBJjz/+eNj5sGjRIkczvgzTCgwfPtxkZ2eHvq6rqzNpaWkmJyfH4aya34IFC8yQIUNcT8MpSWbdunWhr4PBoElJSTEvvvhiaF9JSYnx+/3mlVdecTDD5nHxOhhjzIwZM8y9997rZD6unDp1ykgyeXl5xpgL/+0jIyPNmjVrQmM++eQTI8ns3LnT1TSb3MXrYIwxd9xxh3nqqafcTeoqtPgroPPnz2vv3r0aN25caF9ERITGjRunnTt3OpyZGwcPHlRaWpp69eqlRx99VEePHnU9JacKCwtVVFQUdn7Ex8crMzPzmjw/tm3bpqSkJN1www2aPXu2zp4963pKTSoQCEiSEhISJEl79+5VTU1N2PnQv39/de/evU2fDxevw1dyc3PVpUsXDRw4UPPnz1dFRYWL6V1Wi+uGfbEzZ86orq5OycnJYfuTk5P16aefOpqVG5mZmVqxYoVuuOEGnTx5UgsXLtTtt9+ujz76SLGxsa6n50RRUZEkXfL8+Oqxa8XEiRM1depU9ezZUwUFBfr3f/93ZWVlaefOnWrXrp3r6TW6YDCouXPn6tZbb9XAgQMlXTgfoqKi1Llz57Cxbfl8uNQ6SNIjjzyiHj16KC0tTQcOHNBzzz2n/Px8rV271uFsw7X4AMLfZGVlhf598ODByszMVI8ePbR69Wo99thjDmeGluChhx4K/fugQYM0ePBg9e7dW9u2bdPYsWMdzqxpZGdn66OPPromXge9ksutw6xZs0L/PmjQIKWmpmrs2LEqKChQ7969m3ual9Tin4Lr0qWL2rVrV+8uluLiYqWkpDiaVcvQuXNn9evXT4cOHXI9FWe+Ogc4P+rr1auXunTp0ibPjzlz5mjjxo16++23wz4/LCUlRefPn1dJSUnY+LZ6PlxuHS4lMzNTklrU+dDiAygqKkpDhw7Vli1bQvuCwaC2bNmikSNHOpyZe+fOnVNBQYFSU1NdT8WZnj17KiUlJez8KC0t1XvvvXfNnx/Hjx/X2bNn29T5YYzRnDlztG7dOm3dulU9e/YMe3zo0KGKjIwMOx/y8/N19OjRNnU+NLQOl7J//35Jalnng+u7IK7GqlWrjN/vNytWrDAff/yxmTVrluncubMpKipyPbVm9S//8i9m27ZtprCw0Lz77rtm3LhxpkuXLubUqVOup9akysrKzL59+8y+ffuMJPOzn/3M7Nu3z3z++efGGGN+8pOfmM6dO5sNGzaYAwcOmHvvvdf07NnTVFZWOp5547rSOpSVlZlnnnnG7Ny50xQWFpq33nrL/P3f/73p27evqaqqcj31RjN79mwTHx9vtm3bZk6ePBnaKioqQmOeeOIJ0717d7N161azZ88eM3LkSDNy5EiHs258Da3DoUOHzI9+9COzZ88eU1hYaDZs2GB69eplRo0a5Xjm4VpFABljzC9+8QvTvXt3ExUVZYYPH2527drlekrNbvr06SY1NdVERUWZ66+/3kyfPt0cOnTI9bSa3Ntvv20k1dtmzJhhjLlwK/YPf/hDk5ycbPx+vxk7dqzJz893O+kmcKV1qKioMOPHjzddu3Y1kZGRpkePHubxxx9vc/+TdqmfX5JZvnx5aExlZaX553/+Z3PdddeZjh07mvvuu8+cPHnS3aSbQEPrcPToUTNq1CiTkJBg/H6/6dOnj/nXf/1XEwgE3E78InweEADAiRb/GhAAoG0igAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAn/h8iSRYJtbbfZAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img.squeeze().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5euiuitA-sz",
        "outputId": "38989308-0d23-481a-f5a1-f7150654c645"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img.squeeze().unsqueeze(0).shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLSqzb9PBEyh",
        "outputId": "601aaf54-9b0f-4324-ca97-6a325a51f3f6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在 Python 的 `numpy` 和 `PyTorch`（以及 `TensorFlow`）中，`squeeze()` 是一个非常常用的函数，它的作用是：\n",
        "\n",
        "> **移除形状中为 1 的维度（size为1的维度）**\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 基本用法\n",
        "\n",
        "### NumPy 中：\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([[[1], [2], [3]]])  # shape: (1, 3, 1)\n",
        "x_squeezed = np.squeeze(x)       # shape: (3,)\n",
        "\n",
        "print(x.shape)        # (1, 3, 1)\n",
        "print(x_squeezed.shape)  # (3,)\n",
        "```\n",
        "\n",
        "### PyTorch 中：\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "x = torch.zeros(1, 3, 1)\n",
        "x_squeezed = x.squeeze()\n",
        "\n",
        "print(x.shape)        # torch.Size([1, 3, 1])\n",
        "print(x_squeezed.shape)  # torch.Size([3])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 什么时候会用到 `squeeze()`？\n",
        "\n",
        "### 1. **去掉多余的维度方便计算**\n",
        "\n",
        "很多时候加载数据或某些操作后，张量会多出不必要的 `1` 维度，可能会影响后续的计算，比如：\n",
        "\n",
        "```python\n",
        "y = model(x)  # y shape: (batch_size, 1)\n",
        "loss = loss_fn(y.squeeze(), target)  # 将 shape (batch_size, 1) -> (batch_size,)\n",
        "```\n",
        "\n",
        "### 2. **匹配维度**\n",
        "\n",
        "在计算 loss 或进行张量拼接（concatenate）时，维度不一致会报错，常见的做法就是用 `.squeeze()` 或 `.unsqueeze()` 来调整。\n",
        "\n",
        "### 3. **输出单个值**\n",
        "\n",
        "例如：\n",
        "\n",
        "```python\n",
        "x = torch.tensor([[[42]]])\n",
        "print(x.squeeze())  # tensor(42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 还可以指定要去掉的维度\n",
        "\n",
        "```python\n",
        "x = torch.randn(1, 3, 1)\n",
        "x_squeeze_dim0 = x.squeeze(0)  # 去掉第0维\n",
        "x_squeeze_dim2 = x.squeeze(2)  # 去掉第2维\n",
        "```\n",
        "\n",
        "⚠️ 如果指定的维度不为1，会报错。\n",
        "\n",
        "---\n",
        "\n",
        "## 🚫 注意事项\n",
        "\n",
        "* 只会去掉**维度是1**的部分；\n",
        "* `.squeeze()` 是不改变原张量的，需要赋值或使用 inplace 版本（如 `x.squeeze_()`）；\n",
        "* 可能会不小心把 batch 维度（维度为 1）挤掉，导致后续模型处理出错，所以要小心在什么时候用。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句话：\n",
        "\n",
        "> `squeeze()` 就是在你不想要多余的 1 维时，把它“挤掉”，常用于模型输出、reshape、数据对齐等场景。\n",
        "\n",
        "如果你有某段代码想确认是否需要 `.squeeze()`，可以发给我，我来帮你判断。\n"
      ],
      "metadata": {
        "id": "wIBEG44oBl7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm6y6-MUJGyE",
        "outputId": "c5e1b4cd-3b3c-40b2-ab07-e013e92695ba"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "28*28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4zWH5nUKWSE",
        "outputId": "a35fce07-cfff-4024-8599-5a6bd953b145"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        nn.Linear(512,512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        nn.Linear(512,10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr9W8BQsJ1YI",
        "outputId": "55ad8ad1-eac0-4f30-c14c-3e8d3a8d98c2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.2, inplace=False)\n",
            "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): Dropout(p=0.2, inplace=False)\n",
            "    (8): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "na050sSLMX0i"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| 层             | 作用                             |\n",
        "| ------------- | ------------------------------ |\n",
        "| `Linear`      | 全连接层，产生线性变换                    |\n",
        "| `BatchNorm1d` | 对线性输出做标准化，**稳定训练过程**，减轻内部协变量偏移 |\n",
        "| `ReLU`        | 非线性激活，给模型引入非线性能力               |\n",
        "| `Dropout`     | 在训练时随机丢弃部分激活值，**防止过拟合**        |\n"
      ],
      "metadata": {
        "id": "1x3aN1cVLQay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "当然可以，下面是用**通俗易懂的方式**解释 `BatchNorm1d` 的作用：\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 一句话总结：\n",
        "\n",
        "> **BatchNorm1d 就像是在每一层神经网络“做标准化处理”，帮你把数据“整理整齐”，让网络更容易学得快、学得稳、不容易崩。**\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 为什么要用 BatchNorm？\n",
        "\n",
        "想象你在教一个学生（神经网络）学习做题（训练任务）：\n",
        "\n",
        "* 如果题目（输入）格式很混乱，有的大有的小，有的全是负数，有的突然变大了；\n",
        "* 学生就会很难适应，学习过程会很不稳定，容易学崩、学慢。\n",
        "\n",
        "**BatchNorm 的作用就是把这些题目统一格式**：\n",
        "\n",
        "> 让数据变得“平均值是 0，标准差是 1”（标准化）\n",
        "\n",
        "这样学生就能**快速进入状态，集中精力做题（学习）**，不会因为题目形式变化而“心态崩了”。\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 BatchNorm1d 在干啥？\n",
        "\n",
        "以一句代码为例：\n",
        "\n",
        "```python\n",
        "nn.BatchNorm1d(512)\n",
        "```\n",
        "\n",
        "这个意思是说：对每个 batch 里的样本，在 `512` 维度上分别做如下操作：\n",
        "\n",
        "### 每一维特征都会：\n",
        "\n",
        "1. **计算当前 batch 的均值和方差**\n",
        "2. **把这维的数据标准化** → 减去均值 / 除以标准差\n",
        "3. **再乘一个“可学习的缩放因子” γ，加一个“偏移量” β**\n",
        "\n",
        "最终输出的是一个**标准化后还带有一定灵活性**的特征。\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 举个例子来形象理解：\n",
        "\n",
        "假设你喂给神经网络的数据是这样的：\n",
        "\n",
        "```python\n",
        "[[-100, 0, 300],\n",
        " [ -90, 1, 280],\n",
        " [-110, -1, 310]]\n",
        "```\n",
        "\n",
        "这个差异非常大，模型难学。\n",
        "\n",
        "BatchNorm 就会帮你把每一列**缩放和移动**，变成像这样：\n",
        "\n",
        "```python\n",
        "[[ 0.1, -0.2,  0.5],\n",
        " [-0.1,  0.0,  0.3],\n",
        " [-0.2,  0.2, -0.8]]\n",
        "```\n",
        "\n",
        "现在这些值就“整齐”多了，模型更容易处理。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 优点总结：\n",
        "\n",
        "| 优点        | 类比                  |\n",
        "| --------- | ------------------- |\n",
        "| 训练更快      | 学生面对统一格式的题，刷题效率提升   |\n",
        "| 训练更稳定     | 不容易“梯度爆炸/消失”        |\n",
        "| 减少对初始化的依赖 | 不用太担心初始权重选不好        |\n",
        "| 有轻微正则化效果  | 有点像 Dropout，能稍微防过拟合 |\n",
        "\n",
        "---\n",
        "\n",
        "## 🤔 为什么叫 `BatchNorm1d`？\n",
        "\n",
        "因为它是在**每一个 batch 上做“1D 的特征标准化”**（通常用于 `Linear` 层的输出），所以叫 `1d`。\n",
        "如果是 `Conv2d` 那种图像数据，就要用 `BatchNorm2d`。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句通俗话：\n",
        "\n",
        "> **BatchNorm1d 就是帮你的网络“整顿输入秩序”，标准化每一层的数据，让学习更高效、更稳定，不容易崩。**\n",
        "\n",
        "需要我用图示或动画形式解释，也可以画图给你看。\n"
      ],
      "metadata": {
        "id": "wg_K-MtoLq8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "非常好！你提到了几个深度学习中最关键的部分：**ReLU 激活函数、交叉熵损失函数 CrossEntropyLoss，以及 SGD 优化器**。我会一一给出它们的**公式、图像和作用解释**。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 一、ReLU（Rectified Linear Unit）激活函数\n",
        "\n",
        "### 🔸 公式：\n",
        "\n",
        "$$\n",
        "\\text{ReLU}(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "也就是说：\n",
        "\n",
        "* 如果 $x > 0$，输出就是 $x$\n",
        "* 如果 $x \\leq 0$，输出就是 0\n",
        "\n",
        "### 🔸 图像：\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = np.maximum(0, x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title('ReLU Activation Function')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('ReLU(x)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "（你可以运行上面代码看看图像，也可以我帮你画出图来）\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 作用：\n",
        "\n",
        "* 引入非线性，使神经网络可以学习复杂的函数；\n",
        "* 计算简单，训练快速；\n",
        "* 但可能出现“神经元死亡”问题（ReLU 输出长期为0）。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 二、`nn.CrossEntropyLoss()`\n",
        "\n",
        "### 🔸 用途：\n",
        "\n",
        "这是 **分类任务**中最常用的损失函数，适用于 `model` 的输出是 **logits**（未经过 `softmax` 的得分）。\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 计算过程分为两步（PyTorch 内部自动做的）：\n",
        "\n",
        "1. **先对 logits 做 softmax 得到预测概率：**\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
        "$$\n",
        "\n",
        "其中：\n",
        "\n",
        "* $z_i$：模型输出的第 $i$ 类的 logit\n",
        "* $\\hat{y}_i$：softmax 后的概率\n",
        "\n",
        "2. **再计算交叉熵损失（Cross Entropy）：**\n",
        "\n",
        "$$\n",
        "\\text{Loss} = -\\sum_{i} y_i \\log(\\hat{y}_i)\n",
        "$$\n",
        "\n",
        "其中：\n",
        "\n",
        "* $y_i$ 是 one-hot 标签（只有一个 1 其他都是 0）\n",
        "* 实际计算时只保留正确标签那一项：$-\\log(\\hat{y}_{\\text{true class}})$\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 举个例子：\n",
        "\n",
        "如果你的模型输出是：\n",
        "\n",
        "```python\n",
        "logits = [2.0, 1.0, 0.1]  # 表示三个类别的得分\n",
        "label = 0  # 正确类别是第0类\n",
        "```\n",
        "\n",
        "PyTorch 自动计算：\n",
        "\n",
        "```python\n",
        "softmax = [0.659, 0.242, 0.099]\n",
        "loss = -log(0.659) ≈ 0.417\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 三、`torch.optim.SGD(...)` 随机梯度下降优化器\n",
        "\n",
        "### 🔸 公式：\n",
        "\n",
        "每一轮迭代中，更新参数的公式是：\n",
        "\n",
        "$$\n",
        "\\theta = \\theta - \\eta \\cdot \\nabla_{\\theta} \\mathcal{L}\n",
        "$$\n",
        "\n",
        "其中：\n",
        "\n",
        "* $\\theta$：模型参数\n",
        "* $\\eta$：学习率（如 `1e-3`）\n",
        "* $\\nabla_{\\theta} \\mathcal{L}$：损失函数对参数的梯度\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 你写的代码：\n",
        "\n",
        "```python\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "```\n",
        "\n",
        "意思是：\n",
        "\n",
        "* 用 **交叉熵** 来评估分类误差；\n",
        "* 用 **学习率为 0.001 的 SGD 优化器** 来更新模型参数；\n",
        "* 每个 batch 后执行：\n",
        "\n",
        "```python\n",
        "loss.backward()        # 计算梯度\n",
        "optimizer.step()       # 根据梯度更新参数\n",
        "optimizer.zero_grad()  # 清空上一次的梯度\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结表格：\n",
        "\n",
        "| 项目                   | 公式                                                       | 用途         |\n",
        "| -------------------- | -------------------------------------------------------- | ---------- |\n",
        "| **ReLU**             | $\\max(0, x)$                                             | 激活函数，增加非线性 |\n",
        "| **CrossEntropyLoss** | $-\\log(\\hat{y}_{\\text{true class}})$                     | 分类任务损失     |\n",
        "| **SGD**              | $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta \\mathcal{L}$ | 梯度下降优化参数   |\n",
        "\n",
        "---\n",
        "\n",
        "如果你希望我把 ReLU 和 Softmax 的图画出来展示，也可以告诉我，我可以直接输出图片给你看。是否还想继续了解 Adam 优化器、学习率调度器之类的内容？\n"
      ],
      "metadata": {
        "id": "-q4Z2214NaEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "很好！这两个概念非常关键：**logits** 和 **softmax** 是在分类神经网络中经常遇到的术语。\n",
        "\n",
        "我来用通俗易懂的方式给你讲清楚它们的区别和作用：\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 一、什么是 **logits**？\n",
        "\n",
        "### 🔹一句话定义：\n",
        "\n",
        "> **logits 就是神经网络最后一层的原始输出分数**，还没有经过归一化，也不是概率。\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹举个例子：\n",
        "\n",
        "假设你要做一个 3 分类问题（比如识别猫🐱、狗🐶、鸟🐦）\n",
        "你喂一张图给模型，它最后输出：\n",
        "\n",
        "```python\n",
        "logits = [2.5, 0.3, -1.2]\n",
        "```\n",
        "\n",
        "这就是 logits：\n",
        "\n",
        "* 原始分数\n",
        "* 可以是正的、负的、任意大的\n",
        "* **不满足概率分布**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 二、什么是 **softmax**？\n",
        "\n",
        "### 🔹一句话定义：\n",
        "\n",
        "> **softmax 会把 logits 转换成“概率分布”**，每一类的得分变成了 0\\~1 之间的概率，所有概率加起来是 1。\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹公式：\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
        "$$\n",
        "\n",
        "其中：\n",
        "\n",
        "* $z_i$：logits 中第 i 类的分数\n",
        "* $\\hat{y}_i$：softmax 输出的概率\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹对前面例子的处理：\n",
        "\n",
        "```python\n",
        "logits = [2.5, 0.3, -1.2]\n",
        "```\n",
        "\n",
        "经过 softmax 后：\n",
        "\n",
        "```python\n",
        "softmax_probs = [0.81, 0.15, 0.04]  # 这些就是概率\n",
        "```\n",
        "\n",
        "这意味着：\n",
        "\n",
        "* 模型认为这张图是“猫”的概率是 81%\n",
        "* 是狗的概率是 15%\n",
        "* 是鸟的概率是 4%\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 logits vs softmax：比较总结\n",
        "\n",
        "| 比较项    | logits                  | softmax 结果            |\n",
        "| ------ | ----------------------- | --------------------- |\n",
        "| 值范围    | 任意实数                    | (0, 1) 之间             |\n",
        "| 是否是概率  | ❌ 不是                    | ✅ 是                   |\n",
        "| 是否归一化  | ❌ 否                     | ✅ 是（总和 = 1）           |\n",
        "| 模型输出格式 | 模型最后一层输出                | softmax 是对 logits 的处理 |\n",
        "| 用于哪    | 供 `CrossEntropyLoss` 使用 | 用于推理 / 可视化分类概率        |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ CrossEntropyLoss 自动帮你做了 softmax！\n",
        "\n",
        "你不需要手动 `softmax(logits)`，因为：\n",
        "\n",
        "```python\n",
        "nn.CrossEntropyLoss()\n",
        "```\n",
        "\n",
        "这个函数内部已经**自动对 logits 做了 softmax 计算**，然后才算 loss。\n",
        "\n",
        "所以你只需要把 `logits` 输出给它即可。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句话：\n",
        "\n",
        "> **logits 是模型输出的“原始分数”，softmax 是把这些分数转成概率的函数。**\n",
        "\n",
        "如果你想看 softmax 的图像、或者想自己实现 softmax，我可以也帮你画图或给代码。需要的话说一声！\n"
      ],
      "metadata": {
        "id": "UJaDlfNpNlyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这是一个非常经典的问题！💡**SGD（随机梯度下降）** 和 **Adam（自适应矩估计）** 是两种常用的深度学习优化器，它们的目标都是：\n",
        "\n",
        "> **通过不断更新参数，让模型损失最小化**\n",
        "\n",
        "但它们**更新参数的方式不同**，各有优缺点。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 一张表先概括它们的区别：\n",
        "\n",
        "| 特性         | SGD                  | Adam                        |\n",
        "| ---------- | -------------------- | --------------------------- |\n",
        "| 是否自适应学习率   | ❌ 否                  | ✅ 是（每个参数都有自己的学习率）           |\n",
        "| 是否用动量      | ✅ 可选（SGD + Momentum） | ✅ 自带                        |\n",
        "| 依赖过去梯度的平均值 | ❌ 否                  | ✅ 是（1阶和2阶矩估计）               |\n",
        "| 超参数敏感性     | 较高                   | 较低                          |\n",
        "| 收敛速度       | 慢（但稳）                | 快                           |\n",
        "| 是否容易过拟合    | 较不容易                 | 有时会更容易过拟合                   |\n",
        "| 适合         | 简单任务、大数据、容易泛化        | 稀疏梯度、复杂结构（如RNN、Transformer） |\n",
        "| 典型使用场景     | 经典CNN训练、简洁任务         | NLP、Transformer、预训练模型       |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 一、SGD（Stochastic Gradient Descent）\n",
        "\n",
        "### 📘 公式：\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(\\theta_t)\n",
        "$$\n",
        "\n",
        "* $\\theta$：模型参数\n",
        "* $\\eta$：学习率\n",
        "* $\\nabla_\\theta \\mathcal{L}$：损失对参数的梯度\n",
        "\n",
        "### 🧠 特点：\n",
        "\n",
        "* 每次用一个小批量（batch）数据计算梯度 → 更快迭代；\n",
        "* 容易陷入局部最小值；\n",
        "* 可加 momentum 增强稳定性：\n",
        "\n",
        "$$\n",
        "v_{t+1} = \\mu v_t - \\eta \\cdot \\nabla_\\theta \\mathcal{L} \\\\\n",
        "\\theta_{t+1} = \\theta_t + v_{t+1}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 二、Adam（Adaptive Moment Estimation）\n",
        "\n",
        "### 📘 公式概要：\n",
        "\n",
        "Adam 结合了：\n",
        "\n",
        "* **Momentum**（一阶矩估计，梯度的指数加权平均）；\n",
        "* **RMSProp**（二阶矩估计，平方梯度的平均）；\n",
        "\n",
        "$$\n",
        "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\quad \\text{(一阶动量)} \\\\\n",
        "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\quad \\text{(二阶动量)} \\\\\n",
        "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\quad \\text{(偏差修正)} \\\\\n",
        "\\theta_{t+1} = \\theta_t - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
        "$$\n",
        "\n",
        "### 🧠 特点：\n",
        "\n",
        "* 学习率 **自动适应每个参数**；\n",
        "* 非常适合稀疏梯度的任务（如 NLP 中的词向量训练）；\n",
        "* 初始化不敏感，训练更稳定；\n",
        "* 收敛快，但可能泛化能力差（容易过拟合）；\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 简单理解：\n",
        "\n",
        "| 类比   | SGD           | Adam                  |\n",
        "| ---- | ------------- | --------------------- |\n",
        "| 你走迷宫 | 拿一张纸（当前梯度）照着走 | 手里拿指南针（过去方向）和地图（梯度分布） |\n",
        "| 走法   | 方向直来直去，可能被困   | 会绕路，会加速，会避免重复走错路      |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 实践建议\n",
        "\n",
        "* 🔹 **默认用 Adam**，如果你不确定或者任务很复杂；\n",
        "* 🔹 想要更好的**泛化能力**，可以考虑 **SGD + Momentum**；\n",
        "* 🔹 在预训练阶段用 Adam，在微调阶段换成 SGD 也很常见。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句话：\n",
        "\n",
        "> **Adam 是“聪明的自动驾驶”，跑得快也稳；SGD 是“手动挡老司机”，慢但容易控制，容易泛化。**\n",
        "\n",
        "---\n",
        "\n",
        "如果你想了解它们在具体项目中的表现（比如训练 ResNet、BERT 等模型时的表现），我可以继续补充对比案例。是否需要我演示一段两者对比训练的代码？\n"
      ],
      "metadata": {
        "id": "U_DLN-N9ORjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "非常好！`CrossEntropyLoss` 是分类问题中最常见的损失函数。它结合了 **Softmax + 负对数似然（Negative Log Likelihood）** 两个步骤，适用于**多分类**任务。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 一、CrossEntropyLoss 的计算公式\n",
        "\n",
        "对于一个样本的损失（假设类别总数是 $C$，模型输出 logits 是 $\\mathbf{z} = [z_1, z_2, ..., z_C]$，正确标签是第 $y$ 类）：\n",
        "\n",
        "1. **先做 softmax 得到概率分布**：\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\n",
        "$$\n",
        "\n",
        "2. **再对真实类别 $y$ 计算负对数概率**（交叉熵）：\n",
        "\n",
        "$$\n",
        "\\text{Loss} = -\\log(\\hat{y}_y)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 二、实际例子讲解（手动计算）\n",
        "\n",
        "假设你有 3 个类别，模型输出 logits：\n",
        "\n",
        "```python\n",
        "logits = [2.0, 1.0, 0.1]\n",
        "label = 0  # 正确类别是第 0 类\n",
        "```\n",
        "\n",
        "### 第一步：softmax\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "e^{2.0} &= 7.389 \\\\\n",
        "e^{1.0} &= 2.718 \\\\\n",
        "e^{0.1} &= 1.105 \\\\\n",
        "\\text{总和} &= 7.389 + 2.718 + 1.105 = 11.212 \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "得到 softmax 概率：\n",
        "\n",
        "```python\n",
        "[7.389/11.212, 2.718/11.212, 1.105/11.212] ≈ [0.659, 0.242, 0.099]\n",
        "```\n",
        "\n",
        "### 第二步：交叉熵损失\n",
        "\n",
        "由于正确标签是第 0 类（概率是 0.659）：\n",
        "\n",
        "$$\n",
        "\\text{Loss} = -\\log(0.659) ≈ 0.417\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 三、PyTorch 中的代码实现\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "logits = torch.tensor([[2.0, 1.0, 0.1]])  # shape: [batch_size=1, num_classes=3]\n",
        "labels = torch.tensor([0])  # 正确类别为 class 0\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss = loss_fn(logits, labels)\n",
        "\n",
        "print(loss.item())  # 输出大约 0.417\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ 注意事项\n",
        "\n",
        "* **输入 logits 不要先做 softmax**，PyTorch 的 `CrossEntropyLoss` 会**自动内部处理 softmax**；\n",
        "* `logits` 的 shape 是 `[batch_size, num_classes]`；\n",
        "* `labels` 是整型类别索引（不是 one-hot 向量），shape 是 `[batch_size]`；\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句话：\n",
        "\n",
        "> `CrossEntropyLoss = -log(正确类别的 softmax 概率)`，它度量模型“对正确答案的信心”，越不自信，损失越大。\n",
        "\n",
        "---\n",
        "\n",
        "是否还想看多个样本 batch 下的交叉熵计算？或者 one-hot 标签与交叉熵的关系？我可以继续讲。\n"
      ],
      "metadata": {
        "id": "w2DS0UeFOrdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataloader))\n",
        "len(train_dataloader.dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waSg2mmuO-uN",
        "outputId": "5a00dd38-e607-4e37-c882-a48b31b3b79f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # Compute prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xmnaerDUO1ml"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "非常棒的问题！你提到了两个深度学习训练过程中很关键但容易被忽略的细节：`model.train()` vs `model.eval()` 以及 test 中 loss 的计算。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 一、`model.train()` 和 `model.eval()` 是干嘛的？\n",
        "\n",
        "这两个方法是用于**切换模型的工作模式**的，它们不会直接影响 forward 运算的逻辑，但会影响**某些特定层的行为**，比如：\n",
        "\n",
        "| 模型层类型       | 在 `train()` 模式下行为  | 在 `eval()` 模式下行为 |\n",
        "| ----------- | ------------------ | ---------------- |\n",
        "| `Dropout`   | 每次随机丢掉一部分神经元       | 不进行 Dropout（即不丢） |\n",
        "| `BatchNorm` | 使用当前 batch 统计量做标准化 | 使用训练时保存的全局均值和方差  |\n",
        "\n",
        "### ✳️ 你代码中就有这些层：\n",
        "\n",
        "```python\n",
        "nn.Dropout(0.2),\n",
        "nn.BatchNorm1d(512),\n",
        "```\n",
        "\n",
        "所以：\n",
        "\n",
        "* 在训练时，使用 `model.train()` 是必须的，否则 Dropout 和 BatchNorm 不会生效；\n",
        "* 在测试/验证时，使用 `model.eval()` 是必须的，否则这些层的行为会导致结果不稳定或错误。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 二、关于 test 中 loss 的计算方式\n",
        "\n",
        "你说得没错，test 中每个 batch 的 loss 是不同的。但是代码这样处理：\n",
        "\n",
        "```python\n",
        "test_loss += loss_fn(pred, y).item()  # 累加每个 batch 的 loss\n",
        "...\n",
        "test_loss /= num_batches              # 最后除以 batch 数\n",
        "```\n",
        "\n",
        "这是在**计算所有 batch 的平均 loss（即平均每个 batch 的损失）**，这种写法虽然不是最严格的“平均每个样本的损失”，但**在 batch size 相同的情况下是等价的**。\n",
        "\n",
        "### 🧠 如果你想更严格，可以这样写：\n",
        "\n",
        "```python\n",
        "test_loss += loss_fn(pred, y).item() * X.size(0)  # 按照每个样本加权累加\n",
        "...\n",
        "test_loss /= size  # 最后除以总样本数\n",
        "```\n",
        "\n",
        "这种方式更精确，适合在 batch size 不一致时使用。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句话：\n",
        "\n",
        "* `model.train()`：设置为训练模式，**启用 Dropout 和 BatchNorm 训练行为**；\n",
        "* `model.eval()`：设置为测试模式，**关闭 Dropout，用 BatchNorm 的历史均值**；\n",
        "* 测试 loss 的写法没错，但在 batch size 不一致时建议加权平均。\n",
        "\n",
        "---\n",
        "\n",
        "如果你愿意，我可以帮你写一个完整版本的 `train` + `test` 框架，包括日志记录、早停、可视化等功能。是否需要？\n"
      ],
      "metadata": {
        "id": "JM676vmtT5yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这是一个非常关键的问题，很多初学者容易误解！你问的是：\n",
        "\n",
        "> `loss.item()` 是不是只取了第一个样本的 loss？\n",
        "\n",
        "### ✅ 答案是：**不是！`loss.item()` 是整个 batch 的平均 loss（标量）**\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 详细解释：\n",
        "\n",
        "你在代码中计算了：\n",
        "\n",
        "```python\n",
        "loss = loss_fn(pred, y)  # pred.shape: (batch_size, num_classes)\n",
        "```\n",
        "\n",
        "PyTorch 中默认的 `loss_fn = nn.CrossEntropyLoss()` **返回的是整个 batch 的平均 loss**，也就是说：\n",
        "\n",
        "$$\n",
        "\\text{loss} = \\frac{1}{N} \\sum_{i=1}^N \\text{CrossEntropy}(y_i, \\hat{y}_i)\n",
        "$$\n",
        "\n",
        "其中 $N$ 是当前 batch 的大小。\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 那 `loss.item()` 是做什么的？\n",
        "\n",
        "`loss` 是一个张量（`tensor(0.3874, grad_fn=...)`），它还带着计算图。\n",
        "\n",
        "> `loss.item()` 是把这个张量里的**单个标量值取出来**，变成 Python 的 float，方便你打印、记录或存入日志文件。\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 举个例子：\n",
        "\n",
        "假设你的 batch size 是 64，模型对这 64 个样本都做了预测，`loss_fn` 会返回：\n",
        "\n",
        "```python\n",
        "loss = tensor(0.3472, grad_fn=<...>)\n",
        "```\n",
        "\n",
        "你再执行：\n",
        "\n",
        "```python\n",
        "loss.item()  # → 0.3472（float 类型）\n",
        "```\n",
        "\n",
        "这个数值就是**这 64 个样本的平均 loss**，不是某一个样本的 loss。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 补充：想要每个样本的 loss 怎么做？\n",
        "\n",
        "可以用 `reduction='none'` 的方式：\n",
        "\n",
        "```python\n",
        "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
        "losses = loss_fn(pred, y)  # shape: (batch_size,)\n",
        "```\n",
        "\n",
        "此时：\n",
        "\n",
        "* `losses[i]` 就是第 `i` 个样本的 loss；\n",
        "* 如果你想自己算平均，可以：\n",
        "\n",
        "  ```python\n",
        "  loss = losses.mean()\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句话：\n",
        "\n",
        "> `loss.item()` 是整个 batch 的平均 loss（一个标量），**不是第一个样本的 loss**，它只是把 tensor 转为 float。\n",
        "\n",
        "如果你想要打印每个样本的损失，我可以帮你改一下代码展示。是否需要？\n"
      ],
      "metadata": {
        "id": "7bbDeJXMidY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n -------------------------------\")\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\n",
        "  test(test_dataloader, model, loss_fn)\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScdmarkNT4wm",
        "outputId": "9a52d02e-9c2b-4c96-9bef-cda36bc246aa"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            " -------------------------------\n",
            "loss: 2.351133 [   64/60000]\n",
            "loss: 1.504371 [ 6464/60000]\n",
            "loss: 1.231074 [12864/60000]\n",
            "loss: 1.036299 [19264/60000]\n",
            "loss: 0.953311 [25664/60000]\n",
            "loss: 0.944878 [32064/60000]\n",
            "loss: 0.852010 [38464/60000]\n",
            "loss: 0.914644 [44864/60000]\n",
            "loss: 0.760960 [51264/60000]\n",
            "loss: 0.678545 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.0%, Avg loss: 0.725412 \n",
            "\n",
            "Epoch 2\n",
            " -------------------------------\n",
            "loss: 0.797271 [   64/60000]\n",
            "loss: 0.630035 [ 6464/60000]\n",
            "loss: 0.684512 [12864/60000]\n",
            "loss: 0.679657 [19264/60000]\n",
            "loss: 0.832178 [25664/60000]\n",
            "loss: 0.604194 [32064/60000]\n",
            "loss: 0.648339 [38464/60000]\n",
            "loss: 0.796758 [44864/60000]\n",
            "loss: 0.723518 [51264/60000]\n",
            "loss: 0.660618 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.1%, Avg loss: 0.592749 \n",
            "\n",
            "Epoch 3\n",
            " -------------------------------\n",
            "loss: 0.732579 [   64/60000]\n",
            "loss: 0.669723 [ 6464/60000]\n",
            "loss: 0.673791 [12864/60000]\n",
            "loss: 0.489371 [19264/60000]\n",
            "loss: 0.478027 [25664/60000]\n",
            "loss: 0.461428 [32064/60000]\n",
            "loss: 0.515199 [38464/60000]\n",
            "loss: 0.667646 [44864/60000]\n",
            "loss: 0.694426 [51264/60000]\n",
            "loss: 0.561443 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.530315 \n",
            "\n",
            "Epoch 4\n",
            " -------------------------------\n",
            "loss: 0.626839 [   64/60000]\n",
            "loss: 0.796239 [ 6464/60000]\n",
            "loss: 0.576547 [12864/60000]\n",
            "loss: 0.452355 [19264/60000]\n",
            "loss: 0.582835 [25664/60000]\n",
            "loss: 0.502582 [32064/60000]\n",
            "loss: 0.560854 [38464/60000]\n",
            "loss: 0.438994 [44864/60000]\n",
            "loss: 0.451904 [51264/60000]\n",
            "loss: 0.360498 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.494021 \n",
            "\n",
            "Epoch 5\n",
            " -------------------------------\n",
            "loss: 0.480244 [   64/60000]\n",
            "loss: 0.508736 [ 6464/60000]\n",
            "loss: 0.450043 [12864/60000]\n",
            "loss: 0.537892 [19264/60000]\n",
            "loss: 0.412323 [25664/60000]\n",
            "loss: 0.416140 [32064/60000]\n",
            "loss: 0.386087 [38464/60000]\n",
            "loss: 0.685761 [44864/60000]\n",
            "loss: 0.311524 [51264/60000]\n",
            "loss: 0.407041 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.3%, Avg loss: 0.473002 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Save PyTorch Model State to model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0vu9mByVQFP",
        "outputId": "37e3070a-3189-483c-bbe5-b05b95589330"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save PyTorch Model State to model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading models\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
      ],
      "metadata": {
        "id": "vAeN37NFVaib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x,y = test_data[0]\n",
        "model.to(device)\n",
        "with torch.no_grad():\n",
        "  x =x.to(device)\n",
        "  pred = model(x)\n",
        "  print(pred)\n",
        "  # predicted = classes[pred[0].argmax(0)]\n",
        "  predicted = classes[pred.argmax(1).item()]\n",
        "  actual = classes[y]\n",
        "  print(f\"Predicted: {predicted}, Actual: {actual}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk0k-lL5XPTK",
        "outputId": "fd3198f6-bf9a-4998-e681-6e0a57c60016"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.7596, -1.9728, -1.8500, -1.4932, -2.2793,  2.3234, -1.8076,  3.5825,\n",
            "         -0.0408,  4.3602]], device='cuda:0')\n",
            "Predicted: Ankle boot, Actual: Ankle boot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "很好！让我们逐步拆解 `pred(x)[0].argmax(0)` 的含义，它经常出现在**分类模型预测中**。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 假设背景：\n",
        "\n",
        "你有一个**多分类模型**（比如用于识别手写数字的 MNIST），模型输入一个样本后输出一个向量（logits），例如：\n",
        "\n",
        "```python\n",
        "pred(x)  → tensor([[0.2, -1.0, 2.3, 0.5, ..., -0.6]])  # shape: [1, num_classes]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 逐步解释 `pred(x)[0].argmax(0)`\n",
        "\n",
        "### 🔹 `pred(x)`\n",
        "\n",
        "模型对输入 `x` 的预测输出，形状一般是 `[1, num_classes]`（1 表示 batch size = 1）\n",
        "\n",
        "例如：\n",
        "\n",
        "```python\n",
        "tensor([[0.2, -1.0, 2.3, 0.5, -0.6]])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 `pred(x)[0]`\n",
        "\n",
        "取出第一个（也是唯一一个）样本的预测结果，变成一维 tensor：\n",
        "\n",
        "```python\n",
        "tensor([0.2, -1.0, 2.3, 0.5, -0.6])  # shape: [num_classes]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 `.argmax(0)`\n",
        "\n",
        "在维度 0（因为是一维向量）上取最大值的**索引**（index）：\n",
        "\n",
        "```python\n",
        "argmax(0) → 2  # 因为 2.3 是最大值，它在第 2 个位置\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 最终意义：\n",
        "\n",
        "```python\n",
        "pred(x)[0].argmax(0)\n",
        "```\n",
        "\n",
        "这段代码的作用是：\n",
        "\n",
        "> **得到模型对样本 `x` 的预测类别（即分数最高的类别的索引）**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 示例总结：\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "output = torch.tensor([[0.2, -1.0, 2.3, 0.5, -0.6]])\n",
        "pred_class = output[0].argmax(0)\n",
        "print(pred_class)  # 输出：tensor(2)\n",
        "```\n",
        "\n",
        "代表这个样本的预测类别是类 2。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 扩展建议：\n",
        "\n",
        "如果你有多个样本，建议使用：\n",
        "\n",
        "```python\n",
        "pred_classes = pred(x).argmax(dim=1)\n",
        "```\n",
        "\n",
        "这样可以一次性得到 batch 内所有样本的预测类别。\n",
        "\n",
        "---\n",
        "\n",
        "是否需要我画一张图来说明 argmax 的含义？或者讲讲 `softmax + argmax` 和 `CrossEntropyLoss` 之间的关系？\n"
      ],
      "metadata": {
        "id": "aJ9etjwQX-ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = np.maximum(0, x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title('ReLU Activation Function')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('ReLU(x)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "p1HVAYBTjR3a",
        "outputId": "d80f7086-0e8d-4f68-c33c-229ca2b4493d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Personal recurrent\n",
        "## step1 data handling and dataloader\n",
        "## step2 build model and send it to device\n",
        "## step3 indicate optimizer and loss function\n",
        "## step4 write train and test function\n",
        "## step5 run epochs and check test dataset\n",
        "## step6 save model\n",
        "## step7 load model"
      ],
      "metadata": {
        "id": "6oTGIamyksdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "test_data = FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "id": "kolowIN9lLRF"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "hG8jrZG_q7hB"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-K6QFEbrzlK",
        "outputId": "18485716-f4ee-4ef5-e299-0c2d38bbb33b"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.seq_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        nn.Linear(512, 512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        nn.Linear(512, 10)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.seq_stack(x)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)"
      ],
      "metadata": {
        "id": "SXuadNfysFpb"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)"
      ],
      "metadata": {
        "id": "DsJEYPFjud_D"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.parameters() 是一个可迭代器，里面包含了模型中所有需要更新的参数；\n",
        "\n",
        "PyTorch 优化器（如 SGD, Adam）必须明确你要更新哪些参数。"
      ],
      "metadata": {
        "id": "ABfVjvo7vlpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader.dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD1hAzzgvxDy",
        "outputId": "d3768744-47f0-47f3-924c-dd4a81bd09ae"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  model.train()\n",
        "  total_size = len(dataloader.dataset)\n",
        "  correct = 0\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    pred = model(X)\n",
        "    losses = loss_fn(pred, y)\n",
        "    loss = losses.mean()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "      acc = correct / X.size(0)\n",
        "      correct = 0\n",
        "      print(f\"batch: {batch} loss: {loss} acc: {acc}\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "  model.eval()\n",
        "  total_size = len(dataloader.dataset)\n",
        "  total_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      losses = loss_fn(pred, y)\n",
        "      total_loss += losses.sum().item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  avg_loss = total_loss / total_size\n",
        "  acc = correct / total_size\n",
        "  print(f\"test loss: {avg_loss:.4f} acc: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "2B_IZW6qviOZ"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "非常好的问题！这三句话是 **PyTorch 中训练模型的核心步骤**，负责完成「反向传播 + 参数更新 + 梯度清零」。我们一条一条来解释，每一句都很重要：\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 上下文代码：\n",
        "\n",
        "```python\n",
        "loss.backward()       # ① 反向传播\n",
        "optimizer.step()      # ② 更新模型参数\n",
        "optimizer.zero_grad() # ③ 清空梯度\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ① `loss.backward()`\n",
        "\n",
        "### 👉 意思：\n",
        "\n",
        "计算损失 `loss` 对模型所有参数的**梯度（偏导数）**。\n",
        "\n",
        "### 🧠 原理：\n",
        "\n",
        "PyTorch 使用自动微分系统（autograd），当你调用 `.backward()` 时，它会自动根据损失函数沿着模型反向传播，计算所有参数的梯度（即告诉你：每个参数该往哪个方向调整才能减少 loss）。\n",
        "\n",
        "### 🎯 作用：\n",
        "\n",
        "这一步构建了“梯度信息”，但**还没更新模型参数**。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ② `optimizer.step()`\n",
        "\n",
        "### 👉 意思：\n",
        "\n",
        "根据刚刚 `.backward()` 得到的梯度，**更新模型的参数**。\n",
        "\n",
        "### 🧠 原理（以 Adam 或 SGD 为例）：\n",
        "\n",
        "更新的公式大致是：\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta \\cdot \\nabla_\\theta\n",
        "$$\n",
        "\n",
        "即每个参数 $\\theta$ 会往减少 loss 的方向移动一小步，步长由学习率 $\\eta$ 决定。\n",
        "\n",
        "### 🎯 作用：\n",
        "\n",
        "这是\\*\\*真正“优化模型”\\*\\*的一步，让它表现得更好。\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ ③ `optimizer.zero_grad()`\n",
        "\n",
        "### 👉 意思：\n",
        "\n",
        "把所有模型参数的梯度清空（归零）。\n",
        "\n",
        "### 🧠 原因：\n",
        "\n",
        "* PyTorch 中梯度是 **累加的**（默认不是覆盖，而是 +=）；\n",
        "* 如果你不清零，下一轮训练的梯度会叠加在之前的梯度上，导致错误的更新。\n",
        "\n",
        "```python\n",
        "# 错误示例（没有 zero_grad）：\n",
        "loss.backward()  # 第一次\n",
        "loss.backward()  # 第二次：梯度叠加上去了 ❌\n",
        "```\n",
        "\n",
        "### 🎯 作用：\n",
        "\n",
        "**确保每个 batch 都是“干净地”重新计算梯度。**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 总结一句话记忆：\n",
        "\n",
        "> `loss.backward()` 是**算梯度**，`optimizer.step()` 是**用梯度改参数**，`optimizer.zero_grad()` 是**清空梯度准备下次**。\n",
        "\n",
        "---\n",
        "\n",
        "### 📘 一个完整的典型训练循环：\n",
        "\n",
        "```python\n",
        "for X, y in dataloader:\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    loss.backward()        # ① 计算梯度\n",
        "    optimizer.step()       # ② 用梯度更新参数\n",
        "    optimizer.zero_grad()  # ③ 清除梯度\n",
        "```\n",
        "\n",
        "这三步顺序不能错，必须是 **先 backward → 再 step → 最后 zero\\_grad**。\n",
        "\n",
        "是否还想了解“为什么梯度会累加”背后的实现细节？我可以深入讲 autograd 的内部逻辑。\n"
      ],
      "metadata": {
        "id": "y1MqFxfnx7cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1} \\n==========================\")\n",
        "  train(train_loader, model, loss_fn, optimizer)\n",
        "  test(test_loader, model, loss_fn)\n",
        "\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4B5fqL459I0",
        "outputId": "924fb5c8-4197-4448-efaa-14b0591fa8e4"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \n",
            "==========================\n",
            "batch: 0 loss: 2.3986711502075195 acc: 0.125\n",
            "batch: 100 loss: 0.5554952621459961 acc: 0.84375\n",
            "batch: 200 loss: 0.4497145712375641 acc: 0.828125\n",
            "batch: 300 loss: 0.4333111047744751 acc: 0.875\n",
            "batch: 400 loss: 0.655768096446991 acc: 0.78125\n",
            "batch: 500 loss: 0.7607214450836182 acc: 0.796875\n",
            "batch: 600 loss: 0.6213424801826477 acc: 0.734375\n",
            "batch: 700 loss: 0.4395221173763275 acc: 0.890625\n",
            "batch: 800 loss: 0.442857563495636 acc: 0.828125\n",
            "batch: 900 loss: 0.2911531329154968 acc: 0.90625\n",
            "test loss: 0.4085 acc: 0.8519\n",
            "Epoch 2 \n",
            "==========================\n",
            "batch: 0 loss: 0.2838748097419739 acc: 0.90625\n",
            "batch: 100 loss: 0.23040398955345154 acc: 0.921875\n",
            "batch: 200 loss: 0.5953943729400635 acc: 0.8125\n",
            "batch: 300 loss: 0.21695426106452942 acc: 0.921875\n",
            "batch: 400 loss: 0.2825273275375366 acc: 0.90625\n",
            "batch: 500 loss: 0.4035579562187195 acc: 0.90625\n",
            "batch: 600 loss: 0.28308600187301636 acc: 0.875\n",
            "batch: 700 loss: 0.4439104497432709 acc: 0.859375\n",
            "batch: 800 loss: 0.3554537296295166 acc: 0.921875\n",
            "batch: 900 loss: 0.22876888513565063 acc: 0.9375\n",
            "test loss: 0.3504 acc: 0.8667\n",
            "Epoch 3 \n",
            "==========================\n",
            "batch: 0 loss: 0.32972678542137146 acc: 0.890625\n",
            "batch: 100 loss: 0.3642224371433258 acc: 0.90625\n",
            "batch: 200 loss: 0.5384726524353027 acc: 0.78125\n",
            "batch: 300 loss: 0.2735554575920105 acc: 0.921875\n",
            "batch: 400 loss: 0.35663890838623047 acc: 0.828125\n",
            "batch: 500 loss: 0.25327596068382263 acc: 0.890625\n",
            "batch: 600 loss: 0.3341113328933716 acc: 0.859375\n",
            "batch: 700 loss: 0.3973519802093506 acc: 0.859375\n",
            "batch: 800 loss: 0.30268579721450806 acc: 0.84375\n",
            "batch: 900 loss: 0.22895869612693787 acc: 0.953125\n",
            "test loss: 0.3456 acc: 0.8735\n",
            "Epoch 4 \n",
            "==========================\n",
            "batch: 0 loss: 0.37282681465148926 acc: 0.859375\n",
            "batch: 100 loss: 0.22534510493278503 acc: 0.921875\n",
            "batch: 200 loss: 0.39371564984321594 acc: 0.8125\n",
            "batch: 300 loss: 0.2888581156730652 acc: 0.859375\n",
            "batch: 400 loss: 0.36106398701667786 acc: 0.875\n",
            "batch: 500 loss: 0.18392601609230042 acc: 0.921875\n",
            "batch: 600 loss: 0.22925736010074615 acc: 0.9375\n",
            "batch: 700 loss: 0.1638525426387787 acc: 0.9375\n",
            "batch: 800 loss: 0.27553531527519226 acc: 0.921875\n",
            "batch: 900 loss: 0.28482988476753235 acc: 0.90625\n",
            "test loss: 0.3397 acc: 0.8773\n",
            "Epoch 5 \n",
            "==========================\n",
            "batch: 0 loss: 0.2887543737888336 acc: 0.890625\n",
            "batch: 100 loss: 0.5048924088478088 acc: 0.8125\n",
            "batch: 200 loss: 0.28686046600341797 acc: 0.859375\n",
            "batch: 300 loss: 0.3296695351600647 acc: 0.828125\n",
            "batch: 400 loss: 0.1402270495891571 acc: 0.953125\n",
            "batch: 500 loss: 0.30086854100227356 acc: 0.921875\n",
            "batch: 600 loss: 0.3652258515357971 acc: 0.828125\n",
            "batch: 700 loss: 0.24889135360717773 acc: 0.890625\n",
            "batch: 800 loss: 0.2855384349822998 acc: 0.890625\n",
            "batch: 900 loss: 0.39734095335006714 acc: 0.859375\n",
            "test loss: 0.3413 acc: 0.8716\n",
            "Epoch 6 \n",
            "==========================\n",
            "batch: 0 loss: 0.29055914282798767 acc: 0.859375\n",
            "batch: 100 loss: 0.2647686004638672 acc: 0.9375\n",
            "batch: 200 loss: 0.20634472370147705 acc: 0.9375\n",
            "batch: 300 loss: 0.24468079209327698 acc: 0.921875\n",
            "batch: 400 loss: 0.22255751490592957 acc: 0.9375\n",
            "batch: 500 loss: 0.3101370334625244 acc: 0.890625\n",
            "batch: 600 loss: 0.24188297986984253 acc: 0.890625\n",
            "batch: 700 loss: 0.1422227919101715 acc: 0.984375\n",
            "batch: 800 loss: 0.3150993585586548 acc: 0.859375\n",
            "batch: 900 loss: 0.31475234031677246 acc: 0.890625\n",
            "test loss: 0.3176 acc: 0.8891\n",
            "Epoch 7 \n",
            "==========================\n",
            "batch: 0 loss: 0.3230876922607422 acc: 0.875\n",
            "batch: 100 loss: 0.09051802009344101 acc: 1.0\n",
            "batch: 200 loss: 0.29579704999923706 acc: 0.90625\n",
            "batch: 300 loss: 0.26650184392929077 acc: 0.921875\n",
            "batch: 400 loss: 0.33306315541267395 acc: 0.875\n",
            "batch: 500 loss: 0.3243328630924225 acc: 0.875\n",
            "batch: 600 loss: 0.13174793124198914 acc: 0.953125\n",
            "batch: 700 loss: 0.18873339891433716 acc: 0.921875\n",
            "batch: 800 loss: 0.2631567716598511 acc: 0.890625\n",
            "batch: 900 loss: 0.22636711597442627 acc: 0.90625\n",
            "test loss: 0.3480 acc: 0.8744\n",
            "Epoch 8 \n",
            "==========================\n",
            "batch: 0 loss: 0.28817039728164673 acc: 0.859375\n",
            "batch: 100 loss: 0.26062893867492676 acc: 0.921875\n",
            "batch: 200 loss: 0.20472049713134766 acc: 0.921875\n",
            "batch: 300 loss: 0.34617850184440613 acc: 0.8125\n",
            "batch: 400 loss: 0.2831009030342102 acc: 0.921875\n",
            "batch: 500 loss: 0.24628812074661255 acc: 0.90625\n",
            "batch: 600 loss: 0.32128798961639404 acc: 0.84375\n",
            "batch: 700 loss: 0.22029659152030945 acc: 0.890625\n",
            "batch: 800 loss: 0.33958902955055237 acc: 0.875\n",
            "batch: 900 loss: 0.13760927319526672 acc: 0.953125\n",
            "test loss: 0.3244 acc: 0.8886\n",
            "Epoch 9 \n",
            "==========================\n",
            "batch: 0 loss: 0.25701674818992615 acc: 0.875\n",
            "batch: 100 loss: 0.18450577557086945 acc: 0.875\n",
            "batch: 200 loss: 0.21807610988616943 acc: 0.921875\n",
            "batch: 300 loss: 0.18855024874210358 acc: 0.921875\n",
            "batch: 400 loss: 0.21809479594230652 acc: 0.921875\n",
            "batch: 500 loss: 0.20274117588996887 acc: 0.9375\n",
            "batch: 600 loss: 0.15202388167381287 acc: 0.9375\n",
            "batch: 700 loss: 0.22185777127742767 acc: 0.890625\n",
            "batch: 800 loss: 0.31268760561943054 acc: 0.90625\n",
            "batch: 900 loss: 0.22929446399211884 acc: 0.921875\n",
            "test loss: 0.3265 acc: 0.8852\n",
            "Epoch 10 \n",
            "==========================\n",
            "batch: 0 loss: 0.20761500298976898 acc: 0.890625\n",
            "batch: 100 loss: 0.17233121395111084 acc: 0.9375\n",
            "batch: 200 loss: 0.2597338557243347 acc: 0.921875\n",
            "batch: 300 loss: 0.22364240884780884 acc: 0.953125\n",
            "batch: 400 loss: 0.21830600500106812 acc: 0.9375\n",
            "batch: 500 loss: 0.15750184655189514 acc: 0.9375\n",
            "batch: 600 loss: 0.3331056237220764 acc: 0.859375\n",
            "batch: 700 loss: 0.16708287596702576 acc: 0.90625\n",
            "batch: 800 loss: 0.26742538809776306 acc: 0.921875\n",
            "batch: 900 loss: 0.10223798453807831 acc: 0.9375\n",
            "test loss: 0.3084 acc: 0.8918\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),\"model_state\")\n",
        "print(\"model parameters save to model_state\")b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNpcanTy9VyY",
        "outputId": "b888dadd-1bda-4f59-f8e5-cb666a44612f"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model parameters save to model_state\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "new_model = NeuralNetwork()\n",
        "new_model.load_state_dict(torch.load(\"model_state\"))\n",
        "new_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tksArEiA-AZd",
        "outputId": "331b1518-a951-4a8a-f331-d747f1eac5ce"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (seq_stack): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
              "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.2, inplace=False)\n",
              "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU()\n",
              "    (7): Dropout(p=0.2, inplace=False)\n",
              "    (8): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    }
  ]
}